{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deltalake:\n",
    "* Delta Lake is the optimized storage layer that provides the foundation for table\n",
    "\n",
    "* Supported features\n",
    "\n",
    "  - Schema enforcement and evolution\n",
    "  - **Time travel (Data versoning)**\n",
    "  - Data compaction (Optimize)\n",
    "  - Unified Batch and Streaming Workloads\n",
    "  - **Efficient Upserts and Deletes (MERGE operation)**\n",
    "  - Scalability and Performance\n",
    "  - Data Reliability and Checkpoints \n",
    "  - Compliance and Auditing \n",
    "\n",
    "\n",
    "\n",
    "Demo table:\n",
    "- `customers` - Customer data with CDC history\n",
    "- `products` - Product catalog\n",
    "- `orders` - Order records\n",
    "- `order_items` - Order line items\n",
    "- `cdc_events` - Raw CDC events (audit log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DELTA_LAKE_PATH = \"../deltalake\"  # Delta Lake tables at project root (up one level from notebooks/)\n",
    "\n",
    "# Available tables\n",
    "TABLES = [\"customers\", \"products\", \"orders\", \"order_items\", \"cdc_events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Delta Lake path: /Users/anh.nguyen/Documents/poc/deltalake_poc/deltalake\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Delta Lake path: {os.path.abspath(DELTA_LAKE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def get_table_path(table_name: str) -> str:\n",
    "    \"\"\"Get full path to Delta table.\"\"\"\n",
    "    return os.path.join(DELTA_LAKE_PATH, table_name)\n",
    "\n",
    "def table_exists(table_name: str) -> bool:\n",
    "    \"\"\"Check if Delta table exists.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    return os.path.exists(path) and os.path.exists(os.path.join(path, \"_delta_log\"))\n",
    "\n",
    "def load_table(table_name: str, version: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Load Delta table as pandas DataFrame.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    if version is not None:\n",
    "        dt = DeltaTable(path, version=version)\n",
    "    else:\n",
    "        dt = DeltaTable(path)\n",
    "    return dt.to_pandas()\n",
    "\n",
    "def get_history(table_name: str) -> list:\n",
    "    \"\"\"Get table history.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    dt = DeltaTable(path)\n",
    "    return dt.history()\n",
    "\n",
    "def get_schema(table_name: str) -> dict:\n",
    "    \"\"\"Get table schema.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    dt = DeltaTable(path)\n",
    "    return dt.schema().to_pyarrow()\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Check Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Delta Lake Tables:\n",
      "========================================\n",
      "customers       âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:05.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:05.000Z': error parsing date)\n",
      "products        âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date)\n",
      "orders          âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'order_date': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date)\n",
      "order_items     âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:07.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:07.000Z': error parsing date)\n",
      "cdc_events      âœ… Available (17 rows, 1 versions)\n"
     ]
    }
   ],
   "source": [
    "# Check which tables exist\n",
    "print(\"Available Delta Lake Tables:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for table in TABLES:\n",
    "    exists = table_exists(table)\n",
    "    status = \"âœ… Available\" if exists else \"âŒ Not found\"\n",
    "    \n",
    "    if exists:\n",
    "        try:\n",
    "            df = load_table(table)\n",
    "            row_count = len(df)\n",
    "            history = get_history(table)\n",
    "            version_count = len(history)\n",
    "            print(f\"{table:15} {status} ({row_count} rows, {version_count} versions)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{table:15} {status} (error: {e})\")\n",
    "    else:\n",
    "        print(f\"{table:15} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Query Current Data (delta-rs)\n",
    "\n",
    "**Method:** Python-native `deltalake` library (delta-rs)  \n",
    "**Pros:** Fast, lightweight, no JVM overhead  \n",
    "**Cons:** Limited to basic operations (basic write, read, time travel, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:05.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:05.000Z': error parsing date",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query customers table\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table_exists(\u001b[33m\"\u001b[39m\u001b[33mcustomers\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_customers = \u001b[43mload_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCustomers Table (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_customers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mload_table\u001b[39m\u001b[34m(table_name, version)\u001b[39m\n\u001b[32m     15\u001b[39m     dt = DeltaTable(path, version=version)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     dt = \u001b[43mDeltaTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dt.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/deltalake_poc/lib/python3.14/site-packages/deltalake/table.py:415\u001b[39m, in \u001b[36mDeltaTable.__init__\u001b[39m\u001b[34m(self, table_uri, version, storage_options, without_files, log_buffer_size)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03mCreate the Delta Table from a path with an optional version.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03mMultiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \n\u001b[32m    413\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_options = storage_options\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28mself\u001b[39m._table = \u001b[43mRawDeltaTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mException\u001b[39m: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:05.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:05.000Z': error parsing date"
     ]
    }
   ],
   "source": [
    "# Query customers table\n",
    "if table_exists(\"customers\"):\n",
    "    df_customers = load_table(\"customers\")\n",
    "    print(f\"Customers Table ({len(df_customers)} rows)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_customers)\n",
    "else:\n",
    "    print(\"Customers table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query products table\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table_exists(\u001b[33m\"\u001b[39m\u001b[33mproducts\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_products = \u001b[43mload_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproducts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProducts Table (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_products)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mload_table\u001b[39m\u001b[34m(table_name, version)\u001b[39m\n\u001b[32m     15\u001b[39m     dt = DeltaTable(path, version=version)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     dt = \u001b[43mDeltaTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dt.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/deltalake_poc/lib/python3.14/site-packages/deltalake/table.py:415\u001b[39m, in \u001b[36mDeltaTable.__init__\u001b[39m\u001b[34m(self, table_uri, version, storage_options, without_files, log_buffer_size)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03mCreate the Delta Table from a path with an optional version.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03mMultiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \n\u001b[32m    413\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_options = storage_options\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28mself\u001b[39m._table = \u001b[43mRawDeltaTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mException\u001b[39m: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date"
     ]
    }
   ],
   "source": [
    "# Query products table\n",
    "if table_exists(\"products\"):\n",
    "    df_products = load_table(\"products\")\n",
    "    print(f\"Products Table ({len(df_products)} rows)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_products)\n",
    "else:\n",
    "    print(\"Products table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders Table (3 rows)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>status</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>shipping_address</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>pending</td>\n",
       "      <td>259.97</td>\n",
       "      <td>789 Pine Rd, Chicago, IL 60601</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:20+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>shipped</td>\n",
       "      <td>79.99</td>\n",
       "      <td>456 Oak Ave, Los Angeles, CA 90001</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:19+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>1349.98</td>\n",
       "      <td>123 Main St, New York, NY 10001</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:17+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  customer_id                order_date     status  total_amount  \\\n",
       "0   3            3 1970-01-21 10:41:11+00:00    pending        259.97   \n",
       "1   2            2 1970-01-21 10:41:11+00:00    shipped         79.99   \n",
       "2   1            1 1970-01-21 10:41:11+00:00  completed       1349.98   \n",
       "\n",
       "                     shipping_address                created_at  \\\n",
       "0      789 Pine Rd, Chicago, IL 60601 1970-01-21 10:41:11+00:00   \n",
       "1  456 Oak Ave, Los Angeles, CA 90001 1970-01-21 10:41:11+00:00   \n",
       "2     123 Main St, New York, NY 10001 1970-01-21 10:41:11+00:00   \n",
       "\n",
       "                 updated_at __cdc_operation           __cdc_timestamp  \\\n",
       "0 1970-01-21 10:41:11+00:00               r 2025-12-23 06:29:50+00:00   \n",
       "1 1970-01-21 10:41:11+00:00               r 2025-12-23 06:29:50+00:00   \n",
       "2 1970-01-21 10:41:11+00:00               r 2025-12-23 06:29:50+00:00   \n",
       "\n",
       "             __processed_at  \n",
       "0 2025-12-23 06:43:20+00:00  \n",
       "1 2025-12-23 06:43:19+00:00  \n",
       "2 2025-12-23 06:43:17+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query orders table\n",
    "if table_exists(\"orders\"):\n",
    "    df_orders = load_table(\"orders\")\n",
    "    print(f\"Orders Table ({len(df_orders)} rows)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_orders)\n",
    "else:\n",
    "    print(\"Orders table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Detailed Version Analysis: customers ===\n",
      "\n",
      "Total versions: 6\n",
      "\n",
      "Version Details:\n",
      "====================================================================================================\n",
      "v 5 | 1766472191050 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 4 | 1766472189552 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 3 | 1766472187860 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 2 | 1766472186208 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 1 | 1766472184122 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 0 | 1766472180826 | CREATE TABLE    | Rows: out=N/A, updated=N/A, inserted=N/A\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Key Metrics:\n",
      "- 'numOutputRows': Total rows after operation\n",
      "- 'numTargetRowsUpdated': Rows updated by MERGE\n",
      "- 'numTargetRowsInserted': Rows inserted by MERGE\n"
     ]
    }
   ],
   "source": [
    "# Detailed version analysis - see what changed in each version\n",
    "TABLE_TO_ANALYZE = \"customers\"\n",
    "\n",
    "if table_exists(TABLE_TO_ANALYZE):\n",
    "    print(f\"=== Detailed Version Analysis: {TABLE_TO_ANALYZE} ===\\n\")\n",
    "    \n",
    "    history = get_history(TABLE_TO_ANALYZE)\n",
    "    \n",
    "    print(f\"Total versions: {len(history)}\\n\")\n",
    "    print(\"Version Details:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for entry in history[:15]:  # Show last 15 versions\n",
    "        version = entry.get('version', 'N/A')\n",
    "        timestamp = entry.get('timestamp', 'N/A')\n",
    "        operation = entry.get('operation', 'N/A')\n",
    "        \n",
    "        # Get operation metrics if available\n",
    "        metrics = entry.get('operationMetrics', {})\n",
    "        num_output_rows = metrics.get('numOutputRows', 'N/A')\n",
    "        num_updated_rows = metrics.get('numTargetRowsUpdated', 'N/A')\n",
    "        num_inserted_rows = metrics.get('numTargetRowsInserted', 'N/A')\n",
    "        \n",
    "        print(f\"v{version:2} | {timestamp} | {operation:15} | Rows: out={num_output_rows}, updated={num_updated_rows}, inserted={num_inserted_rows}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    print(\"- 'numOutputRows': Total rows after operation\")\n",
    "    print(\"- 'numTargetRowsUpdated': Rows updated by MERGE\")\n",
    "    print(\"- 'numTargetRowsInserted': Rows inserted by MERGE\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_ANALYZE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Row Count Evolution: customers ===\n",
      "\n",
      "Version  5 | MERGE           | 5 rows | 1766472191050\n",
      "Version  4 | MERGE           | 4 rows | 1766472189552\n",
      "Version  3 | MERGE           | 3 rows | 1766472187860\n",
      "Version  2 | MERGE           | 2 rows | 1766472186208\n",
      "Version  1 | MERGE           | 1 rows | 1766472184122\n",
      "Version  0 | CREATE TABLE    | 0 rows | 1766472180826\n",
      "\n",
      "ðŸ’¡ Observation:\n",
      "   - If row count doesn't change between versions, that MERGE was an UPDATE\n",
      "   - If row count increases, that MERGE added a new record\n"
     ]
    }
   ],
   "source": [
    "# Compare row count across versions to see when records were added\n",
    "TABLE_TO_ANALYZE = \"customers\"\n",
    "\n",
    "if table_exists(TABLE_TO_ANALYZE):\n",
    "    print(f\"=== Row Count Evolution: {TABLE_TO_ANALYZE} ===\\n\")\n",
    "    \n",
    "    history = get_history(TABLE_TO_ANALYZE)\n",
    "    \n",
    "    for entry in history[:15]:\n",
    "        version = entry.get('version', 'N/A')\n",
    "        \n",
    "        try:\n",
    "            df = load_table(TABLE_TO_ANALYZE, version=version)\n",
    "            row_count = len(df)\n",
    "            \n",
    "            # Get operation info\n",
    "            operation = entry.get('operation', 'N/A')\n",
    "            timestamp = entry.get('timestamp', 'N/A')\n",
    "            \n",
    "            print(f\"Version {version:2} | {operation:15} | {row_count} rows | {timestamp}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Version {version:2} | Error: {e}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Observation:\")\n",
    "    print(\"   - If row count doesn't change between versions, that MERGE was an UPDATE\")\n",
    "    print(\"   - If row count increases, that MERGE added a new record\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_ANALYZE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'customers' at Version 0\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, first_name, last_name, email, phone, created_at, updated_at, __cdc_operation, __cdc_timestamp, __processed_at]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time travel: Query specific version\n",
    "TABLE_TO_EXPLORE = \"customers\"\n",
    "VERSION = 0 \n",
    "\n",
    "if table_exists(TABLE_TO_EXPLORE):\n",
    "    try:\n",
    "        df_historical = load_table(TABLE_TO_EXPLORE, version=VERSION)\n",
    "        print(f\"'{TABLE_TO_EXPLORE}' at Version {VERSION}\")\n",
    "        print(\"=\" * 60)\n",
    "        display(df_historical)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading version {VERSION}: {e}\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_EXPLORE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0: 0 rows\n",
      "Latest version: 4 rows\n",
      "\n",
      "------------------------------- Version 0 -------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, first_name, last_name, email, phone, created_at, updated_at, __cdc_operation, __cdc_timestamp, __processed_at]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------- Latest -------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Williams</td>\n",
       "      <td>alice.williams@example.com</td>\n",
       "      <td>+1-555-0104</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>bob.johnson@example.com</td>\n",
       "      <td>+1-555-0103</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>+1-555-0102</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>+1-555-0101</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                       email        phone  \\\n",
       "0   4      Alice  Williams  alice.williams@example.com  +1-555-0104   \n",
       "1   3        Bob   Johnson     bob.johnson@example.com  +1-555-0103   \n",
       "2   2       Jane     Smith      jane.smith@example.com  +1-555-0102   \n",
       "3   1       John       Doe        john.doe@example.com  +1-555-0101   \n",
       "\n",
       "                 created_at                updated_at __cdc_operation  \\\n",
       "0 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "1 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "2 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "3 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "\n",
       "            __cdc_timestamp            __processed_at  \n",
       "0 2025-12-23 06:29:50+00:00 2025-12-23 06:43:08+00:00  \n",
       "1 2025-12-23 06:29:50+00:00 2025-12-23 06:43:06+00:00  \n",
       "2 2025-12-23 06:29:50+00:00 2025-12-23 06:43:04+00:00  \n",
       "3 2025-12-23 06:29:50+00:00 2025-12-23 06:43:00+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare two versions\n",
    "TABLE_TO_COMPARE = \"customers\"\n",
    "VERSION_OLD = 0\n",
    "VERSION_NEW = 4  # None = latest\n",
    "\n",
    "if table_exists(TABLE_TO_COMPARE):\n",
    "    try:\n",
    "        df_old = load_table(TABLE_TO_COMPARE, version=VERSION_OLD)\n",
    "        df_new = load_table(TABLE_TO_COMPARE, version=VERSION_NEW)\n",
    "        \n",
    "        print(f\"Version {VERSION_OLD}: {len(df_old)} rows\")\n",
    "        print(f\"Latest version: {len(df_new)} rows\")\n",
    "        \n",
    "        # Show side by side if small enough\n",
    "        if len(df_old) <= 10 and len(df_new) <= 10:\n",
    "            print(f\"\\n------------------------------- Version {VERSION_OLD} -------------------------------\")\n",
    "            display(df_old)\n",
    "            print(\"\\n------------------------------- Latest -------------------------------\")\n",
    "            display(df_new)\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing versions: {e}\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_COMPARE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Events by Source Table\n",
      "========================================\n",
      "public.products                    5 events\n",
      "public.customers                   5 events\n",
      "public.order_items                 4 events\n",
      "public.orders                      3 events\n"
     ]
    }
   ],
   "source": [
    "# Analyze CDC events by source table\n",
    "if table_exists(\"cdc_events\"):\n",
    "    df_events = load_table(\"cdc_events\")\n",
    "    \n",
    "    print(\"CDC Events by Source Table\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    table_counts = df_events['source_table'].value_counts()\n",
    "    for table, count in table_counts.items():\n",
    "        print(f\"{table:30} {count:5} events\")\n",
    "else:\n",
    "    print(\"CDC events table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent CDC Events (last 10)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>source_table</th>\n",
       "      <th>operation</th>\n",
       "      <th>record_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdc.public.products-0-4</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-23 06:29:50.604000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdc.public.products-0-3</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-23 06:29:50.604000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdc.public.products-0-2</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdc.public.products-0-1</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdc.public.products-0-0</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cdc.public.orders-0-2</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cdc.public.orders-0-1</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cdc.public.orders-0-0</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cdc.public.order_items-0-3</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-23 06:29:50.610000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cdc.public.order_items-0-2</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.609000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      event_id        source_table operation  record_id  \\\n",
       "0      cdc.public.products-0-4     public.products         r          5   \n",
       "1      cdc.public.products-0-3     public.products         r          4   \n",
       "2      cdc.public.products-0-2     public.products         r          3   \n",
       "3      cdc.public.products-0-1     public.products         r          2   \n",
       "4      cdc.public.products-0-0     public.products         r          1   \n",
       "5        cdc.public.orders-0-2       public.orders         r          3   \n",
       "6        cdc.public.orders-0-1       public.orders         r          2   \n",
       "11       cdc.public.orders-0-0       public.orders         r          1   \n",
       "9   cdc.public.order_items-0-3  public.order_items         r          4   \n",
       "16  cdc.public.order_items-0-2  public.order_items         r          3   \n",
       "\n",
       "                    event_timestamp  \n",
       "0  2025-12-23 06:29:50.604000+00:00  \n",
       "1  2025-12-23 06:29:50.604000+00:00  \n",
       "2  2025-12-23 06:29:50.603000+00:00  \n",
       "3  2025-12-23 06:29:50.603000+00:00  \n",
       "4  2025-12-23 06:29:50.603000+00:00  \n",
       "5  2025-12-23 06:29:50.607000+00:00  \n",
       "6  2025-12-23 06:29:50.607000+00:00  \n",
       "11 2025-12-23 06:29:50.607000+00:00  \n",
       "9  2025-12-23 06:29:50.610000+00:00  \n",
       "16 2025-12-23 06:29:50.609000+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View recent CDC events\n",
    "if table_exists(\"cdc_events\"):\n",
    "    df_events = load_table(\"cdc_events\")\n",
    "    \n",
    "    print(\"Recent CDC Events (last 10)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort by processed_at if available, otherwise by event_id\n",
    "    if 'processed_at' in df_events.columns:\n",
    "        df_recent = df_events.sort_values('processed_at', ascending=False).head(10)\n",
    "    else:\n",
    "        df_recent = df_events.tail(10)\n",
    "    \n",
    "    display(df_recent[['event_id', 'source_table', 'operation', 'record_id', 'event_timestamp']])\n",
    "else:\n",
    "    print(\"CDC events table not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PySpark SQL Queries\n",
    "\n",
    "**Method:** PySpark with Delta Lake SQL  \n",
    "**Pros:** Full SQL support, complex transformations, aggregations  \n",
    "**Cons:** Requires JVM, heavier resource usage  \n",
    "**Python 3.14 Note:** Use SQL-based operations, avoid `createDataFrame()` due to serialization issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/deltalake_poc/lib/python3.14/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/anh.nguyen/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/anh.nguyen/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9ef23fc6-f580-4e35-9a52-1938efe8b98b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 97ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9ef23fc6-f580-4e35-9a52-1938efe8b98b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/12/24 11:07:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/24 11:07:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created!\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark with Delta Lake\n",
    "USE_SPARK = True  # Set to True to enable PySpark\n",
    "\n",
    "if USE_SPARK:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta import configure_spark_with_delta_pip\n",
    "\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"DeltaLakeNotebook\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    )\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    print(\"SparkSession created!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "else:\n",
    "    print(\"PySpark disabled. Set USE_SPARK = True to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ PySpark SQL Query ------------------------ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      Test|    User1|test.user@example...|+1-555-0101|+57949-09-15 11:3...|+57949-09-19 10:1...|              u|2025-12-24 11:06:...|2025-12-24 11:06:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Total customers: 5\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: Query with SQL\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    # Load Delta tables\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    if table_exists(\"customers\"):\n",
    "        # Method 1: Read as DataFrame\n",
    "        df_spark = spark.read.format(\"delta\").load(customers_path)\n",
    "        df_spark.createOrReplaceTempView(\"customers\")\n",
    "        \n",
    "        # Method 2: Run SQL query\n",
    "        print(\"------------------------ PySpark SQL Query ------------------------ \")\n",
    "        result = spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM customers\n",
    "            ORDER BY id\n",
    "        \"\"\")\n",
    "        result.show()\n",
    "        \n",
    "        print(f\"\\nTotal customers: {result.count()}\")\n",
    "    else:\n",
    "        print(\"Customers table not found.\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------  PySpark Time Travel ------------------------\n",
      "\n",
      "Version 0 (Initial Snapshot):\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      John|      Doe|john.doe@example.com|+1-555-0101|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Latest Version:\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      Test|    User1|test.user@example...|+1-555-0101|+57949-09-15 11:3...|+57949-09-19 10:1...|              u|2025-12-24 11:06:...|2025-12-24 11:06:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Version 0 rows: 5\n",
      "Latest rows: 5\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: Time travel query\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    if table_exists(\"customers\"):\n",
    "        print(\"------------------------  PySpark Time Travel ------------------------\\n\")\n",
    "        \n",
    "        # Query version 0 (first snapshot)\n",
    "        print(\"Version 0 (Initial Snapshot):\")\n",
    "        df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(customers_path)\n",
    "        df_v0.show()\n",
    "        \n",
    "        # Query latest version\n",
    "        print(\"\\nLatest Version:\")\n",
    "        df_latest = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(customers_path)\n",
    "        df_latest.show()\n",
    "        \n",
    "        print(f\"\\nVersion 0 rows: {df_v0.count()}\")\n",
    "        print(f\"Latest rows: {df_latest.count()}\")\n",
    "    else:\n",
    "        print(\"Customers table not found.\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Delta Table History (Real CDC Data) -------------------------------\n",
      "\n",
      "Showing version history for 'customers' table:\n",
      "\n",
      "+-------+-----------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------+-----------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1      |2025-12-24 11:06:36.392|MERGE    |{numTargetRowsCopied -> 4, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3346, numTargetBytesRemoved -> 3204, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2811, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1682, numTargetRowsUpdated -> 1, numOutputRows -> 5, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 1, numSourceRows -> 1, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 510}|\n",
      "|0      |2025-12-24 11:05:52.398|WRITE    |{numFiles -> 2, numOutputRows -> 5, numOutputBytes -> 4341}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+-------+-----------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: View Delta table history (on actual CDC tables)\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    from delta import DeltaTable\n",
    "    \n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    print(\"------------------------------- Delta Table History (Real CDC Data) -------------------------------\\n\")\n",
    "    print(\"Showing version history for 'customers' table:\\n\")\n",
    "    \n",
    "    dt = DeltaTable.forPath(spark, customers_path)\n",
    "    dt.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time Travel: Compare Customer Versions ===\n",
      "\n",
      "Available versions: 0 to 1\n",
      "\n",
      "----------------------------- Version 0 (Initial) -----------------------------\n",
      "Row count: 5\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      John|      Doe|john.doe@example.com|+1-555-0101|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57949-09-15 11:3...|+57949-09-15 11:3...|              r|2025-12-24 11:02:...|2025-12-24 11:05:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "----------------------------- Latest Version (v1) -----------------------------\n",
      "Error: Cannot time travel Delta table to version 6. Available versions: [0, 1].\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Compare versions of customers table (Time Travel)\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    print(\"=== Time Travel: Compare Customer Versions ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get version count\n",
    "        from delta import DeltaTable\n",
    "        dt = DeltaTable.forPath(spark, customers_path)\n",
    "        history = dt.history().select(\"version\").collect()\n",
    "        max_version = max([row.version for row in history])\n",
    "        \n",
    "        print(f\"Available versions: 0 to {max_version}\\n\")\n",
    "        \n",
    "        # Compare version 0 vs latest\n",
    "        print(\"----------------------------- Version 0 (Initial) -----------------------------\")\n",
    "        df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(customers_path)\n",
    "        print(f\"Row count: {df_v0.count()}\")\n",
    "        df_v0.show(5)\n",
    "        \n",
    "        print(f\"\\n----------------------------- Latest Version (v{max_version}) -----------------------------\")\n",
    "        df_v6 = spark.read.format(\"delta\").option(\"versionAsOf\", 6).load(customers_path)\n",
    "        print(f\"Row count: {df_v0.count()}\")\n",
    "        df_v6.show(5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time Travel by Timestamp: Version 5 vs Version 6 ===\n",
      "\n",
      "Version 0 timestamp: 2025-12-24 11:05:52.398000\n",
      "Version 1 timestamp: 2025-12-24 11:06:36.392000\n",
      "\n",
      "----------------------------- Version 0 -----------------------------\n",
      "Row count: 5\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|id |first_name|last_name|email                     |phone      |created_at           |updated_at           |__cdc_operation|__cdc_timestamp        |__processed_at         |\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|1  |John      |Doe      |john.doe@example.com      |+1-555-0101|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.804|2025-12-24 11:05:44.769|\n",
      "|2  |Jane      |Smith    |jane.smith@example.com    |+1-555-0102|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.806|2025-12-24 11:05:44.769|\n",
      "|3  |Bob       |Johnson  |bob.johnson@example.com   |+1-555-0103|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.806|2025-12-24 11:05:44.769|\n",
      "|4  |Alice     |Williams |alice.williams@example.com|+1-555-0104|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.807|2025-12-24 11:05:44.769|\n",
      "|5  |Charlie   |Brown    |charlie.brown@example.com |+1-555-0105|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.807|2025-12-24 11:05:44.769|\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "\n",
      "----------------------------- Version 1 -----------------------------\n",
      "Row count: 5\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|id |first_name|last_name|email                     |phone      |created_at           |updated_at           |__cdc_operation|__cdc_timestamp        |__processed_at         |\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|1  |Test      |User1    |test.user@example.com     |+1-555-0101|+57949-09-15 11:39:04|+57949-09-19 10:10:44|u              |2025-12-24 11:06:26.428|2025-12-24 11:06:30.03 |\n",
      "|2  |Jane      |Smith    |jane.smith@example.com    |+1-555-0102|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.806|2025-12-24 11:05:44.769|\n",
      "|3  |Bob       |Johnson  |bob.johnson@example.com   |+1-555-0103|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.806|2025-12-24 11:05:44.769|\n",
      "|4  |Alice     |Williams |alice.williams@example.com|+1-555-0104|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.807|2025-12-24 11:05:44.769|\n",
      "|5  |Charlie   |Brown    |charlie.brown@example.com |+1-555-0105|+57949-09-15 11:39:04|+57949-09-15 11:39:04|r              |2025-12-24 11:02:20.807|2025-12-24 11:05:44.769|\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "customers_path = get_table_path(\"customers\")\n",
    "\n",
    "print(\"=== Time Travel by Timestamp: Version 5 vs Version 6 ===\\n\")\n",
    "\n",
    "# Load Delta table\n",
    "dt = DeltaTable.forPath(spark, customers_path)\n",
    "\n",
    "# Get history with timestamps\n",
    "history_df = (\n",
    "    dt.history(10)\n",
    "      .select(\"version\", \"timestamp\", \"operation\")\n",
    "      .orderBy(\"version\")\n",
    ")\n",
    "\n",
    "# history_df.show(truncate=False)\n",
    "\n",
    "# Extract timestamps for v5 and v6\n",
    "history = history_df.collect()\n",
    "\n",
    "ts_v5 = next(row.timestamp for row in history if row.version == 0)\n",
    "ts_v6 = next(row.timestamp for row in history if row.version == 1)\n",
    "\n",
    "print(f\"Version 0 timestamp: {ts_v5}\")\n",
    "print(f\"Version 1 timestamp: {ts_v6}\\n\")\n",
    "\n",
    "# Read data AS OF version 0 (by timestamp)\n",
    "df_v5 = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"timestampAsOf\", ts_v5)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# Read data AS OF version 6 (by timestamp)\n",
    "df_v6 = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"timestampAsOf\", ts_v6)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# Compare\n",
    "print(\"----------------------------- Version 0 -----------------------------\")\n",
    "print(f\"Row count: {df_v5.count()}\")\n",
    "df_v5.show(5, truncate=False)\n",
    "\n",
    "print(\"----------------------------- Version 1 -----------------------------\")\n",
    "print(f\"Row count: {df_v6.count()}\")\n",
    "df_v6.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---------------------+----------------+---------------+-----------------------+\n",
      "|id |first_name|last_name|email                |_change_type    |_commit_version|_commit_timestamp      |\n",
      "+---+----------+---------+---------------------+----------------+---------------+-----------------------+\n",
      "|1  |John      |Doe      |john.doe@example.com |insert          |0              |2025-12-24 11:05:52.398|\n",
      "|1  |John      |Doe      |john.doe@example.com |update_preimage |1              |2025-12-24 11:06:36.392|\n",
      "|1  |Test      |User1    |test.user@example.com|update_postimage|1              |2025-12-24 11:06:36.392|\n",
      "+---+----------+---------+---------------------+----------------+---------------+-----------------------+\n",
      "\n",
      "\n",
      "Total changes tracked: 3\n"
     ]
    }
   ],
   "source": [
    "# Track all changes on a specific customer record using Change Data Feed\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    from delta import DeltaTable\n",
    "    \n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    try:\n",
    "        # Enable Change Data Feed on the table (if not already enabled)\n",
    "        dt = DeltaTable.forPath(spark, customers_path)\n",
    "        \n",
    "        # all changes (insert, update, delete) for each record\n",
    "        changes_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .load(customers_path)\n",
    "        )\n",
    "        \n",
    "        # changes on a specific customer (ID = 1)\n",
    "        customer_id = 1\n",
    "        customer_changes = changes_df.filter(f\"id = {customer_id}\").orderBy(\"_commit_version\")\n",
    "        \n",
    "        # Show key columns including change metadata\n",
    "        customer_changes.select(\n",
    "            \"id\", \n",
    "            \"first_name\", \n",
    "            \"last_name\", \n",
    "            \"email\",\n",
    "            \"_change_type\",      # insert, update_preimage, update_postimage, delete\n",
    "            \"_commit_version\",   # Delta version number\n",
    "            \"_commit_timestamp\"  # When change occurred\n",
    "        ).show(truncate=False)\n",
    "        \n",
    "        print(f\"\\nTotal changes tracked: {customer_changes.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID 1 - Final States Only (cleaner view):\n",
      "====================================================================================================\n",
      "+---------------+----------------+---+----------+---------+---------------------+-----------------------+\n",
      "|_commit_version|_change_type    |id |first_name|last_name|email                |_commit_timestamp      |\n",
      "+---------------+----------------+---+----------+---------+---------------------+-----------------------+\n",
      "|0              |insert          |1  |John      |Doe      |john.doe@example.com |2025-12-24 11:05:52.398|\n",
      "|1              |update_postimage|1  |Test      |User1    |test.user@example.com|2025-12-24 11:06:36.392|\n",
      "+---------------+----------------+---+----------+---------+---------------------+-----------------------+\n",
      "\n",
      "\n",
      "âœ“ This shows only the 'after' state of each change\n",
      "  Total state changes: 2\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    \n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    try:\n",
    "        changes_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .load(customers_path)\n",
    "        )\n",
    "        \n",
    "        customer_id = 1\n",
    "        \n",
    "        final_states = changes_df.filter(\n",
    "            f\"id = {customer_id} AND _change_type IN ('insert', 'update_postimage', 'delete')\"\n",
    "        ).orderBy(\"_commit_version\")\n",
    "        \n",
    "        print(f\"Customer ID {customer_id} - Final States Only (cleaner view):\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        final_states.select(\n",
    "            \"_commit_version\",\n",
    "            \"_change_type\",\n",
    "            \"id\", \n",
    "            \"first_name\", \n",
    "            \"last_name\", \n",
    "            \"email\",\n",
    "            \"_commit_timestamp\"\n",
    "        ).show(truncate=False)\n",
    "        \n",
    "        print(f\"\\nâœ“ This shows only the 'after' state of each change\")\n",
    "        print(f\"  Total state changes: {final_states.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-12-23 15:11:58.029|NULL  |NULL    |MERGE    |{predicate -> [\"(id#5370L = id#4704L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3092, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 2888, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2066, numTargetRowsUpdated -> 0, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 131}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.2.0|\n",
      "|0      |2025-12-23 15:06:29.64 |NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 2, numOutputRows -> 5, numOutputBytes -> 4342}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forPath(spark, customers_path)\n",
    "history_df = dt.history()\n",
    "\n",
    "history_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Product Popularity: Most Ordered Items -----------------------------\n",
      "\n",
      "+-------------------+-----------+-------------+-------------------+\n",
      "|       product_name|   category|times_ordered|total_quantity_sold|\n",
      "+-------------------+-----------+-------------+-------------------+\n",
      "|Mechanical Keyboard|Electronics|            1|                  2|\n",
      "|          USB-C Hub|Electronics|            1|                  1|\n",
      "|         Laptop Pro|Electronics|            1|                  1|\n",
      "|     Wireless Mouse|Electronics|            1|                  1|\n",
      "|      Monitor Stand|     Office|            0|               NULL|\n",
      "|       Test Product|       Test|            0|               NULL|\n",
      "+-------------------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Product analysis with order items\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"products\") and table_exists(\"order_items\"):\n",
    "    products_path = get_table_path(\"products\")\n",
    "    items_path = get_table_path(\"order_items\")\n",
    "    \n",
    "    df_products = spark.read.format(\"delta\").load(products_path)\n",
    "    df_items = spark.read.format(\"delta\").load(items_path)\n",
    "    \n",
    "    df_products.createOrReplaceTempView(\"products\")\n",
    "    df_items.createOrReplaceTempView(\"order_items\")\n",
    "    \n",
    "    print(\"----------------------------- Product Popularity: Most Ordered Items -----------------------------\\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.name as product_name,\n",
    "            p.category,\n",
    "            COUNT(oi.id) as times_ordered,\n",
    "            SUM(oi.quantity) as total_quantity_sold\n",
    "        FROM products p\n",
    "        LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "        GROUP BY p.id, p.name, p.category\n",
    "        ORDER BY total_quantity_sold DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or tables not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Order Analysis: Items per Order -----------------------------\n",
      "\n",
      "+--------+-----------+-------------------+----------+--------------+\n",
      "|order_id|customer_id|         order_date|item_count|total_quantity|\n",
      "+--------+-----------+-------------------+----------+--------------+\n",
      "|       1|          1|1970-01-21 18:41:11|         2|             2|\n",
      "|       3|          3|1970-01-21 18:41:11|         1|             2|\n",
      "|       2|          2|1970-01-21 18:41:11|         1|             1|\n",
      "+--------+-----------+-------------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Analyze order details\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"orders\") and table_exists(\"order_items\"):\n",
    "    orders_path = get_table_path(\"orders\")\n",
    "    items_path = get_table_path(\"order_items\")\n",
    "    \n",
    "    df_orders = spark.read.format(\"delta\").load(orders_path)\n",
    "    df_items = spark.read.format(\"delta\").load(items_path)\n",
    "    \n",
    "    df_orders.createOrReplaceTempView(\"orders\")\n",
    "    df_items.createOrReplaceTempView(\"order_items\")\n",
    "    \n",
    "    print(\"----------------------------- Order Analysis: Items per Order -----------------------------\\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            o.id as order_id,\n",
    "            o.customer_id,\n",
    "            o.order_date,\n",
    "            COUNT(oi.id) as item_count,\n",
    "            SUM(oi.quantity) as total_quantity\n",
    "        FROM orders o\n",
    "        LEFT JOIN order_items oi ON o.id = oi.order_id\n",
    "        GROUP BY o.id, o.customer_id, o.order_date\n",
    "        ORDER BY total_quantity DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or tables not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JOIN: Customers with Their Orders ===\n",
      "\n",
      "+-----------+----------+---------+--------------------+------------+\n",
      "|customer_id|first_name|last_name|               email|total_orders|\n",
      "+-----------+----------+---------+--------------------+------------+\n",
      "|          3|       Bob|  Johnson|bob.johnson@examp...|           1|\n",
      "|          2|      Jane|    Smith|jane.smith@exampl...|           1|\n",
      "|          1|      John|      Doe|john.doe@example.com|           1|\n",
      "|          4|     Alice| Williams|alice.williams@ex...|           0|\n",
      "|          5|   Charlie|    Brown|charlie.brown@exa...|           0|\n",
      "+-----------+----------+---------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Join customers with orders\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\") and table_exists(\"orders\"):\n",
    "    # Load both tables\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    orders_path = get_table_path(\"orders\")\n",
    "    \n",
    "    df_customers = spark.read.format(\"delta\").load(customers_path)\n",
    "    df_orders = spark.read.format(\"delta\").load(orders_path)\n",
    "    \n",
    "    df_customers.createOrReplaceTempView(\"customers\")\n",
    "    df_orders.createOrReplaceTempView(\"orders\")\n",
    "    \n",
    "    print(\"=== JOIN: Customers with Their Orders ===\\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            c.id as customer_id,\n",
    "            c.first_name,\n",
    "            c.last_name,\n",
    "            c.email,\n",
    "            COUNT(o.id) as total_orders\n",
    "        FROM customers c\n",
    "        LEFT JOIN orders o ON c.id = o.customer_id\n",
    "        GROUP BY c.id, c.first_name, c.last_name, c.email\n",
    "        ORDER BY total_orders DESC\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo: Create Table and Insert Records ===\n",
      "\n",
      "Step 1: Creating new Delta table...\n",
      "Table created!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo: Create a new Delta table\n",
    "\n",
    "demo_table_path = \"/Users/anh.nguyen/Documents/poc/deltalake_poc/deltalake/demo_employees\"\n",
    "demo_table_name = \"demo_employees\"\n",
    "\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:\n",
    "        # Drop table if exist\n",
    "        spark.sql(f\"\"\"\n",
    "                DROP TABLE IF EXISTS {demo_table_name}\n",
    "                  \"\"\")    \n",
    "\n",
    "        # 1: Create new Delta table\n",
    "        print(\"Creating new Delta table...\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {demo_table_name} (\n",
    "                id BIGINT NOT NULL,\n",
    "                name STRING NOT NULL,\n",
    "                department STRING,\n",
    "                salary DECIMAL(10, 2),\n",
    "                hire_date DATE,\n",
    "                is_active BOOLEAN\n",
    "            ) USING DELTA\n",
    "            LOCATION '{demo_table_path}'\n",
    "        \"\"\")\n",
    "        print(\"Table created!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Inserting initial records...\n",
      "3 records inserted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo: Insert records using PySpark SQL\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "        \n",
    "    try:        \n",
    "        # 2: Insert initial records\n",
    "        print(\"Inserting initial records...\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {demo_table_name} VALUES\n",
    "            (1, 'Alice Johnson', 'Engineering', 95000.00, DATE '2020-01-15', true),\n",
    "            (2, 'Bob Smith', 'Marketing', 75000.00, DATE '2021-03-20', true),\n",
    "            (3, 'Charlie Brown', 'Engineering', 88000.00, DATE '2019-07-10', true)\n",
    "        \"\"\")\n",
    "        print(\"records inserted!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Viewing inserted data:\n",
      "+---+-------------+------------+---------+----------+---------+\n",
      "| id|         name|  department|   salary| hire_date|is_active|\n",
      "+---+-------------+------------+---------+----------+---------+\n",
      "|  3|Charlie Brown| Engineering| 88000.00|2019-07-10|     true|\n",
      "|  1|Alice Johnson| Engineering|105000.00|2020-01-15|     true|\n",
      "|  1|Alice Johnson| Engineering| 95000.00|2020-01-15|     true|\n",
      "|  5| Eve Anderson| Engineering| 92000.00|2021-11-08|     true|\n",
      "|  3|Charlie Brown|Data Science| 98000.00|2019-07-10|     true|\n",
      "|  6| Frank Miller|       Sales| 78000.00|2023-01-15|     true|\n",
      "|  2|    Bob Smith|   Marketing| 75000.00|2021-03-20|     true|\n",
      "|  2|    Bob Smith|   Marketing| 75000.00|2021-03-20|    false|\n",
      "|  4| Diana Prince|       Sales| 82000.00|2022-05-12|     true|\n",
      "+---+-------------+------------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "\n",
    "    try:                \n",
    "        # 3: View the data\n",
    "        print(\"Viewing inserted data:\")\n",
    "        spark.sql(f\"SELECT * FROM {demo_table_name}\").show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: Inserting additional records...\n",
      "2 more records inserted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:        \n",
    "        # 4: Insert more records\n",
    "        print(\"\\n4: Inserting additional records...\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {demo_table_name} VALUES\n",
    "            (4, 'Diana Prince', 'Sales', 82000.00, DATE '2022-05-12', true),\n",
    "            (5, 'Eve Anderson', 'Engineering', 92000.00, DATE '2021-11-08', true)\n",
    "        \"\"\")\n",
    "        print(\"2 more records inserted!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Viewing all records:\n",
      "Total records: 5\n",
      "+---+-------------+-----------+--------+----------+---------+\n",
      "| id|         name| department|  salary| hire_date|is_active|\n",
      "+---+-------------+-----------+--------+----------+---------+\n",
      "|  1|Alice Johnson|Engineering|95000.00|2020-01-15|     true|\n",
      "|  2|    Bob Smith|  Marketing|75000.00|2021-03-20|     true|\n",
      "|  3|Charlie Brown|Engineering|88000.00|2019-07-10|     true|\n",
      "|  4| Diana Prince|      Sales|82000.00|2022-05-12|     true|\n",
      "|  5| Eve Anderson|Engineering|92000.00|2021-11-08|     true|\n",
      "+---+-------------+-----------+--------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:        \n",
    "        # 5: View updated data\n",
    "        print(\"Step 5: Viewing all records:\")\n",
    "        result = spark.sql(f\"SELECT * FROM {demo_table_name} ORDER BY id\")\n",
    "        print(f\"Total records: {result.count()}\")\n",
    "        result.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Aggregation - Average salary by department:\n",
      "+-----------+--------------+------------+-------------+\n",
      "| department|employee_count|  avg_salary|earliest_hire|\n",
      "+-----------+--------------+------------+-------------+\n",
      "|Engineering|             3|91666.666667|   2019-07-10|\n",
      "|      Sales|             1|82000.000000|   2022-05-12|\n",
      "|  Marketing|             1|75000.000000|   2021-03-20|\n",
      "+-----------+--------------+------------+-------------+\n",
      "\n",
      "\n",
      "Demo complete! Table created at: /Users/anh.nguyen/Documents/poc/deltalake_poc/deltalake/demo_employees\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:        \n",
    "        # 6: Perform aggregation\n",
    "        print(\"\\nStep 6: Aggregation - Average salary by department:\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                department,\n",
    "                COUNT(*) as employee_count,\n",
    "                AVG(salary) as avg_salary,\n",
    "                MIN(hire_date) as earliest_hire\n",
    "            FROM {demo_table_name}\n",
    "            WHERE is_active = true\n",
    "            GROUP BY department\n",
    "            ORDER BY avg_salary DESC\n",
    "        \"\"\").show()\n",
    "        \n",
    "        print(\"\\nDemo complete! Table created at:\", demo_table_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo: MERGE (Upsert) Operation ===\n",
      "\n",
      "Step 1: Performing MERGE operation...\n",
      "  - Update Charlie's department and salary\n",
      "  - Insert new employee Frank\n",
      "\n",
      "âœ… MERGE complete!\n",
      "\n",
      "ðŸ“ Changes:\n",
      "   - Charlie: department 'Engineering' â†’ 'Data Science', salary $88,000 â†’ $98,000\n",
      "   - Frank: NEW employee added\n",
      "\n",
      "Updated table:\n",
      "+---+-------------+------------+--------+----------+---------+\n",
      "| id|         name|  department|  salary| hire_date|is_active|\n",
      "+---+-------------+------------+--------+----------+---------+\n",
      "|  1|Alice Johnson| Engineering|95000.00|2020-01-15|     true|\n",
      "|  2|    Bob Smith|   Marketing|75000.00|2021-03-20|     true|\n",
      "|  3|Charlie Brown|Data Science|98000.00|2019-07-10|     true|\n",
      "|  4| Diana Prince|       Sales|82000.00|2022-05-12|     true|\n",
      "|  5| Eve Anderson| Engineering|92000.00|2021-11-08|     true|\n",
      "|  6| Frank Miller|       Sales|78000.00|2023-01-15|     true|\n",
      "+---+-------------+------------+--------+----------+---------+\n",
      "\n",
      "\n",
      "Total employees: 6\n"
     ]
    }
   ],
   "source": [
    "# Demo: MERGE (Upsert) operation\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "\n",
    "    if os.path.exists(demo_table_path):\n",
    "        print(\"=== Demo: MERGE (Upsert) Operation ===\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Load table\n",
    "            df = spark.read.format(\"delta\").load(demo_table_path)\n",
    "            df.createOrReplaceTempView(demo_table_name)\n",
    "            \n",
    "            print(\"1: Performing MERGE operation...\")\n",
    "            print(\"  - Update Charlie's department and salary\")\n",
    "            print(\"  - Insert new employee Frank\\n\")\n",
    "            \n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {demo_table_name} AS target\n",
    "                USING (\n",
    "                    SELECT 3 as id, 'Charlie Brown' as name, 'Data Science' as department, \n",
    "                           98000.00 as salary, DATE '2019-07-10' as hire_date, true as is_active\n",
    "                    UNION ALL\n",
    "                    SELECT 6 as id, 'Frank Miller' as name, 'Sales' as department,\n",
    "                           78000.00 as salary, DATE '2023-01-15' as hire_date, true as is_active\n",
    "                ) AS source\n",
    "                ON target.id = source.id\n",
    "                WHEN MATCHED THEN UPDATE SET\n",
    "                    target.department = source.department,\n",
    "                    target.salary = source.salary\n",
    "                WHEN NOT MATCHED THEN INSERT (id, name, department, salary, hire_date, is_active)\n",
    "                    VALUES (source.id, source.name, source.department, source.salary, source.hire_date, source.is_active)\n",
    "            \"\"\")\n",
    "            \n",
    "            # View updated data\n",
    "            print(\"Updated table:\")\n",
    "            df_merged = spark.read.format(\"delta\").load(demo_table_path)\n",
    "            df_merged.orderBy(\"id\").show()\n",
    "            \n",
    "            print(f\"\\nTotal employees: {df_merged.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(\"Demo table not found. Run the 'Create Table' demo first.\")\n",
    "else:\n",
    "    print(\"Spark not enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark: Stop session when done\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    # Uncomment to stop Spark session\n",
    "    # spark.stop()\n",
    "    # print(\"SparkSession stopped.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. MinIO/S3 Delta Lake\n",
    "\n",
    "**Method:** Query Delta Lake tables stored in MinIO (S3-compatible object storage)  \n",
    "**Use Case:** RisingWave CDC pipeline sinks data to Delta Lake in MinIO  \n",
    "**Libraries:** DuckDB (recommended), delta-rs with S3 storage options\n",
    "\n",
    "> **Note:** This section requires MinIO running with RisingWave pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO Configuration:\n",
      "  Endpoint: http://localhost:9000\n",
      "  Bucket: deltalake\n",
      "  Tables: 7\n"
     ]
    }
   ],
   "source": [
    "# MinIO/S3 Configuration\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "MINIO_BUCKET = \"deltalake\"\n",
    "MINIO_REGION = \"us-east-1\"\n",
    "\n",
    "# Storage options for delta-rs\n",
    "S3_STORAGE_OPTIONS = {\n",
    "    \"AWS_ENDPOINT_URL\": MINIO_ENDPOINT,\n",
    "    \"AWS_ACCESS_KEY_ID\": MINIO_ACCESS_KEY,\n",
    "    \"AWS_SECRET_ACCESS_KEY\": MINIO_SECRET_KEY,\n",
    "    \"AWS_REGION\": MINIO_REGION,\n",
    "    \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n",
    "    \"AWS_ALLOW_HTTP\": \"true\",\n",
    "}\n",
    "\n",
    "# Available tables in MinIO (from RisingWave pipeline)\n",
    "S3_TABLES = [\"customers\", \"products\", \"orders\", \"order_items\", \n",
    "             \"order_analytics\", \"customer_order_summary\", \"product_inventory\"]\n",
    "\n",
    "print(\"MinIO Configuration:\")\n",
    "print(f\"  Endpoint: {MINIO_ENDPOINT}\")\n",
    "print(f\"  Bucket: {MINIO_BUCKET}\")\n",
    "print(f\"  Tables: {len(S3_TABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Read with DuckDB (Recommended)\n",
    "\n",
    "DuckDB provides excellent compatibility for reading Delta Lake parquet files from S3/MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB configured for MinIO access\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "# Configure DuckDB for MinIO/S3 access\n",
    "\n",
    "# Create connection with S3\n",
    "duck_conn = duckdb.connect()\n",
    "    \n",
    "# Install and load httpfs extension for S3 access\n",
    "duck_conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "    \n",
    "# Configure S3 settings for MinIO\n",
    "duck_conn.execute(f\"\"\"\n",
    "    SET s3_endpoint = 'localhost:9000';\n",
    "    SET s3_access_key_id = '{MINIO_ACCESS_KEY}';\n",
    "    SET s3_secret_access_key = '{MINIO_SECRET_KEY}';\n",
    "    SET s3_use_ssl = false;\n",
    "    SET s3_url_style = 'path';\n",
    "\"\"\")\n",
    "    \n",
    "print(\"DuckDB configured for MinIO access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query functions defined: s3_query(), s3_table_info()\n"
     ]
    }
   ],
   "source": [
    "def s3_query(table_name: str, sql: str = None) -> \"duckdb.DuckDBPyRelation\":\n",
    "    \"\"\"Query a Delta Lake table from MinIO using DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table in MinIO\n",
    "        sql: Optional SQL query (uses {table} placeholder)\n",
    "    \n",
    "    Returns:\n",
    "        DuckDB relation (use .df() to convert to pandas)\n",
    "    \"\"\"\n",
    "    parquet_path = f\"s3://{MINIO_BUCKET}/{table_name}/*.parquet\"\n",
    "    \n",
    "    if sql:\n",
    "        # Replace {table} with actual parquet path\n",
    "        full_sql = sql.replace(\"{table}\", f\"read_parquet('{parquet_path}')\")\n",
    "        return duck_conn.execute(full_sql)\n",
    "    else:\n",
    "        return duck_conn.execute(f\"SELECT * FROM read_parquet('{parquet_path}')\")\n",
    "\n",
    "\n",
    "def s3_table_info(table_name: str):\n",
    "    \"\"\"Get table info (row count, columns) from MinIO.\"\"\"\n",
    "    try:\n",
    "        result = s3_query(table_name)\n",
    "        df = result.df()\n",
    "        print(f\"Table: {table_name}\")\n",
    "        print(f\"  Rows: {len(df)}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {table_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Lake Tables in MinIO\n",
      "--------------------------------------------------\n",
      "customers: IO Error: No files found that match the pattern \"s\n",
      "products: 12 rows, 8 columns\n",
      "orders: 10 rows, 7 columns\n",
      "order_items: IO Error: No files found that match the pattern \"s\n",
      "order_analytics: 4 rows, 5 columns\n",
      "customer_order_summary: 4 rows, 6 columns\n",
      "product_inventory: 2 rows, 6 columns\n"
     ]
    }
   ],
   "source": [
    "# List all Delta Lake tables in MinIO\n",
    "print(\"Delta Lake Tables in MinIO\")\n",
    "print(\"-\" * 50)\n",
    "    \n",
    "for table in S3_TABLES:\n",
    "    try:\n",
    "        df = s3_query(table).df()\n",
    "        print(f\"{table}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table}: {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>+1-555-0102</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>+1-555-0101</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Williams</td>\n",
       "      <td>alice.williams@example.com</td>\n",
       "      <td>+1-555-0104</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Brown</td>\n",
       "      <td>charlie.brown@example.com</td>\n",
       "      <td>+1-555-0105</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>bob.johnson@example.com</td>\n",
       "      <td>+1-555-0103</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "      <td>2025-12-24 10:37:50.012000+07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                       email        phone  \\\n",
       "0   2       Jane     Smith      jane.smith@example.com  +1-555-0102   \n",
       "1   1       John       Doe        john.doe@example.com  +1-555-0101   \n",
       "2   4      Alice  Williams  alice.williams@example.com  +1-555-0104   \n",
       "3   5    Charlie     Brown   charlie.brown@example.com  +1-555-0105   \n",
       "4   3        Bob   Johnson     bob.johnson@example.com  +1-555-0103   \n",
       "\n",
       "                        created_at                       updated_at  \n",
       "0 2025-12-24 10:37:50.012000+07:00 2025-12-24 10:37:50.012000+07:00  \n",
       "1 2025-12-24 10:37:50.012000+07:00 2025-12-24 10:37:50.012000+07:00  \n",
       "2 2025-12-24 10:37:50.012000+07:00 2025-12-24 10:37:50.012000+07:00  \n",
       "3 2025-12-24 10:37:50.012000+07:00 2025-12-24 10:37:50.012000+07:00  \n",
       "4 2025-12-24 10:37:50.012000+07:00 2025-12-24 10:37:50.012000+07:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query customers table\n",
    "df = s3_query(\"customers\").df()\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>stock_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Laptop Pro</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>1299.99</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Laptop Pro</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>1299.99</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Laptop Pro</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>1299.99</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mechanical Keyboard</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>129.99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mechanical Keyboard</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>129.99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>USB-C Hub</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>79.99</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>USB-C Hub</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>79.99</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Monitor Stand</td>\n",
       "      <td>Office</td>\n",
       "      <td>59.99</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Monitor Stand</td>\n",
       "      <td>Office</td>\n",
       "      <td>59.99</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>49.99</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 name     category    price  stock_quantity\n",
       "0   6           Laptop Pro  Electronics  1299.99              50\n",
       "1   1           Laptop Pro  Electronics  1299.99              50\n",
       "2   1           Laptop Pro  Electronics  1299.99              50\n",
       "3   4  Mechanical Keyboard  Electronics   129.99             100\n",
       "4   4  Mechanical Keyboard  Electronics   129.99             100\n",
       "5   3            USB-C Hub  Electronics    79.99             150\n",
       "6   3            USB-C Hub  Electronics    79.99             150\n",
       "7   5        Monitor Stand       Office    59.99              75\n",
       "8   5        Monitor Stand       Office    59.99              75\n",
       "9   2       Wireless Mouse  Electronics    49.99             200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Custom SQL query on MinIO data\n",
    "\n",
    "result = s3_query(\"products\", \"\"\"\n",
    "    SELECT id, name, category, price, stock_quantity\n",
    "    FROM {table}\n",
    "    ORDER BY price DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "display(result.df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_day</th>\n",
       "      <th>status</th>\n",
       "      <th>order_count</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_order_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>completed</td>\n",
       "      <td>2</td>\n",
       "      <td>2699.96</td>\n",
       "      <td>1349.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>pending</td>\n",
       "      <td>1</td>\n",
       "      <td>259.97</td>\n",
       "      <td>259.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaT</td>\n",
       "      <td>shipped</td>\n",
       "      <td>1</td>\n",
       "      <td>79.99</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaT</td>\n",
       "      <td>completed</td>\n",
       "      <td>1</td>\n",
       "      <td>1349.98</td>\n",
       "      <td>1349.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_day     status  order_count  total_revenue  avg_order_value\n",
       "0       NaT  completed            2        2699.96          1349.98\n",
       "1       NaT    pending            1         259.97           259.97\n",
       "2       NaT    shipped            1          79.99            79.99\n",
       "3       NaT  completed            1        1349.98          1349.98"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query analytics materialized view from RisingWave\n",
    "try:\n",
    "    result = s3_query(\"order_analytics\", \"\"\"\n",
    "        SELECT \n",
    "            order_day,\n",
    "            status,\n",
    "            order_count,\n",
    "            total_revenue,\n",
    "            avg_order_value\n",
    "        FROM {table}\n",
    "        ORDER BY order_day DESC\n",
    "        LIMIT 10\n",
    "        \"\"\")\n",
    "    display(result.df())\n",
    "except Exception as e:\n",
    "    print(f\"No data yet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Read with delta-rs\n",
    "\n",
    "delta-rs can also read from S3/MinIO with proper storage options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta-rs S3 functions defined: load_s3_table(), s3_table_history()\n"
     ]
    }
   ],
   "source": [
    "# Read Delta Lake table from MinIO using delta-rs\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "def load_s3_table(table_name: str, version: int = None):\n",
    "    \"\"\"Load a Delta Lake table from MinIO using delta-rs.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table in MinIO\n",
    "        version: Optional version number for time travel\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame\n",
    "    \"\"\"\n",
    "    table_path = f\"s3://{MINIO_BUCKET}/{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        if version is not None:\n",
    "            dt = DeltaTable(table_path, storage_options=S3_STORAGE_OPTIONS, version=version)\n",
    "        else:\n",
    "            dt = DeltaTable(table_path, storage_options=S3_STORAGE_OPTIONS)\n",
    "        \n",
    "        return dt.to_pandas()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def s3_table_history(table_name: str):\n",
    "    \"\"\"Get version history of a Delta Lake table from MinIO.\"\"\"\n",
    "    table_path = f\"s3://{MINIO_BUCKET}/{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        dt = DeltaTable(table_path, storage_options=S3_STORAGE_OPTIONS)\n",
    "        return dt.history()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting history for {table_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>bob.johnson@example.com</td>\n",
       "      <td>+1-555-0103</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>+1-555-0101</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Williams</td>\n",
       "      <td>alice.williams@example.com</td>\n",
       "      <td>+1-555-0104</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Brown</td>\n",
       "      <td>charlie.brown@example.com</td>\n",
       "      <td>+1-555-0105</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>+1-555-0102</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                       email        phone  \\\n",
       "0   3        Bob   Johnson     bob.johnson@example.com  +1-555-0103   \n",
       "1   1       John       Doe        john.doe@example.com  +1-555-0101   \n",
       "2   4      Alice  Williams  alice.williams@example.com  +1-555-0104   \n",
       "3   5    Charlie     Brown   charlie.brown@example.com  +1-555-0105   \n",
       "4   2       Jane     Smith      jane.smith@example.com  +1-555-0102   \n",
       "\n",
       "                        created_at                       updated_at  \n",
       "0 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "1 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "2 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "3 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "4 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read customers table with delta-rs\n",
    "df = load_s3_table(\"customers\")\n",
    "if df is not None:\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1: 1766547674147 - WRITE\n",
      "Version 0: 1766547541257 - CREATE TABLE\n"
     ]
    }
   ],
   "source": [
    "# View table version history\n",
    "history = s3_table_history(\"customers\")\n",
    "if history:\n",
    "    for h in history[:5]:  # Show last 5 versions\n",
    "        print(f\"Version {h.get('version')}: {h.get('timestamp')} - {h.get('operation')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0: 0 rows\n",
      "Latest: 5 rows\n",
      "Row difference: +5\n",
      "\n",
      "--- Version 0 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, first_name, last_name, email, phone, created_at, updated_at]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Latest ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>bob.johnson@example.com</td>\n",
       "      <td>+1-555-0103</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>+1-555-0101</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Williams</td>\n",
       "      <td>alice.williams@example.com</td>\n",
       "      <td>+1-555-0104</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Brown</td>\n",
       "      <td>charlie.brown@example.com</td>\n",
       "      <td>+1-555-0105</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>+1-555-0102</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "      <td>2025-12-24 03:37:50.012000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                       email        phone  \\\n",
       "0   3        Bob   Johnson     bob.johnson@example.com  +1-555-0103   \n",
       "1   1       John       Doe        john.doe@example.com  +1-555-0101   \n",
       "2   4      Alice  Williams  alice.williams@example.com  +1-555-0104   \n",
       "3   5    Charlie     Brown   charlie.brown@example.com  +1-555-0105   \n",
       "4   2       Jane     Smith      jane.smith@example.com  +1-555-0102   \n",
       "\n",
       "                        created_at                       updated_at  \n",
       "0 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "1 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "2 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "3 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  \n",
       "4 2025-12-24 03:37:50.012000+00:00 2025-12-24 03:37:50.012000+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ New records added (IDs): [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Compare two versions of a table in MinIO (Time Travel)\n",
    "TABLE_TO_COMPARE = \"customers\"\n",
    "VERSION_OLD = 0\n",
    "VERSION_NEW = None  # None = latest\n",
    "\n",
    "try:\n",
    "    # Load old version\n",
    "    df_old = load_s3_table(TABLE_TO_COMPARE, version=VERSION_OLD)\n",
    "    \n",
    "    # Load latest version (or specific version)\n",
    "    df_new = load_s3_table(TABLE_TO_COMPARE, version=VERSION_NEW)\n",
    "    \n",
    "    if df_old is not None and df_new is not None:\n",
    "        print(f\"Version {VERSION_OLD}: {len(df_old)} rows\")\n",
    "        print(f\"{'Latest' if VERSION_NEW is None else f'Version {VERSION_NEW}'}: {len(df_new)} rows\")\n",
    "        print(f\"Row difference: {len(df_new) - len(df_old):+d}\\n\")\n",
    "        \n",
    "        # Show version 0\n",
    "        print(f\"--- Version {VERSION_OLD} ---\")\n",
    "        display(df_old.head(5))\n",
    "        \n",
    "        # Show latest\n",
    "        print(f\"\\n--- {'Latest' if VERSION_NEW is None else f'Version {VERSION_NEW}'} ---\")\n",
    "        display(df_new.head(5))\n",
    "        \n",
    "        # Find new records (IDs in new but not in old)\n",
    "        if 'id' in df_old.columns and 'id' in df_new.columns:\n",
    "            new_ids = set(df_new['id']) - set(df_old['id'])\n",
    "            if new_ids:\n",
    "                print(f\"\\nâœ¨ New records added (IDs): {sorted(new_ids)}\")\n",
    "            \n",
    "            deleted_ids = set(df_old['id']) - set(df_new['id'])\n",
    "            if deleted_ids:\n",
    "                print(f\"ðŸ—‘ï¸  Records removed (IDs): {sorted(deleted_ids)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error comparing versions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 10)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id  â”† first_name â”† last_name â”† email      â”† â€¦ â”† updated_at â”† _change_ty â”† _commit_ve â”† _commit_t â”‚\n",
      "â”‚ --- â”† ---        â”† ---       â”† ---        â”†   â”† ---        â”† pe         â”† rsion      â”† imestamp  â”‚\n",
      "â”‚ i32 â”† str        â”† str       â”† str        â”†   â”† datetime[Î¼ â”† ---        â”† ---        â”† ---       â”‚\n",
      "â”‚     â”†            â”†           â”†            â”†   â”† s, UTC]    â”† str        â”† i64        â”† datetime[ â”‚\n",
      "â”‚     â”†            â”†           â”†            â”†   â”†            â”†            â”†            â”† ms]       â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 1   â”† John       â”† Doe       â”† john.doe@e â”† â€¦ â”† 2025-12-24 â”† insert     â”† 1          â”† 2025-12-2 â”‚\n",
      "â”‚     â”†            â”†           â”† xample.com â”†   â”† 03:37:50.0 â”†            â”†            â”† 4 03:41:1 â”‚\n",
      "â”‚     â”†            â”†           â”†            â”†   â”† 12 UTC     â”†            â”†            â”† 4.147     â”‚\n",
      "â”‚ 1   â”† Test       â”† User1     â”† test.user@ â”† â€¦ â”† 2025-12-24 â”† insert     â”† 2          â”† 2025-12-2 â”‚\n",
      "â”‚     â”†            â”†           â”† example.co â”†   â”† 03:51:43.6 â”†            â”†            â”† 4 03:51:5 â”‚\n",
      "â”‚     â”†            â”†           â”† m          â”†   â”† 08 UTC     â”†            â”†            â”† 1.867     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Track history changes using Change Data Feed (CDF)\n",
    "import polars as pl\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "TABLE_NAME = \"customers\"\n",
    "RECORD_ID = 1\n",
    "\n",
    "table_path = f\"s3://{MINIO_BUCKET}/{TABLE_NAME}\"\n",
    "dt = DeltaTable(table_path, storage_options=S3_STORAGE_OPTIONS)\n",
    "\n",
    "arrow_reader = dt.load_cdf(\n",
    "    starting_version=0,\n",
    "    ending_version=None,  # up to latest\n",
    ")\n",
    "cdf = pl.from_arrow(arrow_reader.read_all())\n",
    "\n",
    "# Filter for a specific record\n",
    "record_history = (\n",
    "    cdf\n",
    "    .filter(pl.col(\"id\") == RECORD_ID)\n",
    "    .sort(\"_commit_version\")  # or \"_commit_timestamp\"\n",
    ")\n",
    "\n",
    "print(record_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Write to MinIO Delta Lake\n",
    "\n",
    "Write data to Delta Lake tables in MinIO using delta-rs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write function defined: write_s3_table()\n"
     ]
    }
   ],
   "source": [
    "from deltalake import write_deltalake\n",
    "import pyarrow as pa\n",
    "\n",
    "def write_s3_table(table_name: str, df, mode: str = \"append\"):\n",
    "    \"\"\"Write a pandas DataFrame to Delta Lake in MinIO.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table in MinIO\n",
    "        df: pandas DataFrame to write\n",
    "        mode: 'append' or 'overwrite'\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful\n",
    "    \"\"\"\n",
    "    table_path = f\"s3://{MINIO_BUCKET}/{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        write_deltalake(\n",
    "            table_path,\n",
    "            df,\n",
    "            mode=mode,\n",
    "            storage_options=S3_STORAGE_OPTIONS,\n",
    "        )\n",
    "        print(f\"Wrote {len(df)} rows to {table_name} (mode={mode})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to {table_name}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3 rows to test_table (mode=overwrite)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Test A</td>\n",
       "      <td>100.50</td>\n",
       "      <td>2025-12-24 09:50:44.510982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Test B</td>\n",
       "      <td>200.75</td>\n",
       "      <td>2025-12-24 09:50:44.510982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Test C</td>\n",
       "      <td>300.25</td>\n",
       "      <td>2025-12-24 09:50:44.510982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    name   value                 created_at\n",
       "0   1  Test A  100.50 2025-12-24 09:50:44.510982\n",
       "1   2  Test B  200.75 2025-12-24 09:50:44.510982\n",
       "2   3  Test C  300.25 2025-12-24 09:50:44.510982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Write a new test table to MinIO\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create sample data\n",
    "test_data = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Test A\", \"Test B\", \"Test C\"],\n",
    "    \"value\": [100.50, 200.75, 300.25],\n",
    "    \"created_at\": [datetime.now()] * 3\n",
    "})\n",
    "\n",
    "# Write to new table\n",
    "write_s3_table(\"test_table\", test_data, mode=\"overwrite\")\n",
    "    \n",
    "# Verify\n",
    "df = load_s3_table(\"test_table\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close DuckDB connection when done\n",
    "duck_conn.close()\n",
    "print(\"DuckDB connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Query Delta Lake with Apache Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Delta Lake support\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"DeltaLake-MinIO-Query\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # S3/MinIO configuration\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # Delta Lake packages\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"io.delta:delta-spark_2.12:3.2.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying with Spark: An error occurred while calling o143.load.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.delta.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:191)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource$.parsePathIdentifier(DeltaDataSource.scala:354)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.x$1$lzycompute(DeltaTableV2.scala:75)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.x$1(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelByPath$lzycompute(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelByPath(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$timeTravelSpec$1(DeltaTableV2.scala:108)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelSpec$lzycompute(DeltaTableV2.scala:108)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelSpec(DeltaTableV2.scala:104)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:127)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 46 more\n",
      "\n",
      "\n",
      "Note: Ensure Spark has the required Delta Lake and Hadoop AWS JARs\n"
     ]
    }
   ],
   "source": [
    "# Query Delta tables from MinIO using Spark SQL\n",
    "TABLE_NAME = \"customers\"\n",
    "table_path = f\"s3a://{MINIO_BUCKET}/{TABLE_NAME}\"\n",
    "\n",
    "try:\n",
    "    # Read Delta table\n",
    "    df = spark.read.format(\"delta\").load(table_path)\n",
    "    \n",
    "    print(f\"=== {TABLE_NAME} table via Spark ===\")\n",
    "    print(f\"Total rows: {df.count()}\")\n",
    "    print(f\"Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Run SQL query\n",
    "    df.createOrReplaceTempView(TABLE_NAME)\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT id, first_name, last_name, email \n",
    "        FROM {TABLE_NAME} \n",
    "        ORDER BY id \n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    result.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error querying with Spark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDF not available: An error occurred while calling o150.load.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.delta.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:191)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource$.parsePathIdentifier(DeltaDataSource.scala:354)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.x$1$lzycompute(DeltaTableV2.scala:75)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.x$1(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelByPath$lzycompute(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelByPath(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$timeTravelSpec$1(DeltaTableV2.scala:108)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelSpec$lzycompute(DeltaTableV2.scala:108)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelSpec(DeltaTableV2.scala:104)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:127)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 46 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Change Data Feed using Spark (if CDF is enabled)\n",
    "try:\n",
    "    cdf_df = (spark.read.format(\"delta\")\n",
    "        .option(\"readChangeFeed\", \"true\")\n",
    "        .option(\"startingVersion\", 0)\n",
    "        .load(table_path)\n",
    "    )\n",
    "    print(f\"Total change records: {cdf_df.count()}\")\n",
    "    \n",
    "    # Show changes with metadata columns\n",
    "    cdf_df.select(\n",
    "        \"id\", \"first_name\", \"last_name\",\n",
    "        \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    "    ).orderBy(\"_commit_version\", \"id\").show(20, truncate=False)\n",
    "    \n",
    "    # Summary by change type\n",
    "    print(\"\\nChanges by type:\")\n",
    "    cdf_df.groupBy(\"_change_type\").count().show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CDF not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltalake_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
