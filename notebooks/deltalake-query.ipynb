{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deltalake:\n",
    "* Delta Lake is the optimized storage layer that provides the foundation for table\n",
    "\n",
    "* Supported features\n",
    "\n",
    "  - Schema enforcement and evolution\n",
    "  - **Time travel (Data versoning)**\n",
    "  - Data compaction (Optimize)\n",
    "  - Unified Batch and Streaming Workloads\n",
    "  - **Efficient Upserts and Deletes (MERGE operation)**\n",
    "  - Scalability and Performance\n",
    "  - Data Reliability and Checkpoints \n",
    "  - Compliance and Auditing \n",
    "\n",
    "\n",
    "\n",
    "Demo table:\n",
    "- `customers` - Customer data with CDC history\n",
    "- `products` - Product catalog\n",
    "- `orders` - Order records\n",
    "- `order_items` - Order line items\n",
    "- `cdc_events` - Raw CDC events (audit log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DELTA_LAKE_PATH = \"../deltalake\"  # Delta Lake tables at project root (up one level from notebooks/)\n",
    "\n",
    "# Available tables\n",
    "TABLES = [\"customers\", \"products\", \"orders\", \"order_items\", \"cdc_events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Delta Lake path: /Users/anh.nguyen/Documents/poc/deltalake_poc/deltalake\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Delta Lake path: {os.path.abspath(DELTA_LAKE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def get_table_path(table_name: str) -> str:\n",
    "    \"\"\"Get full path to Delta table.\"\"\"\n",
    "    return os.path.join(DELTA_LAKE_PATH, table_name)\n",
    "\n",
    "def table_exists(table_name: str) -> bool:\n",
    "    \"\"\"Check if Delta table exists.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    return os.path.exists(path) and os.path.exists(os.path.join(path, \"_delta_log\"))\n",
    "\n",
    "def load_table(table_name: str, version: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Load Delta table as pandas DataFrame.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    if version is not None:\n",
    "        dt = DeltaTable(path, version=version)\n",
    "    else:\n",
    "        dt = DeltaTable(path)\n",
    "    return dt.to_pandas()\n",
    "\n",
    "def get_history(table_name: str) -> list:\n",
    "    \"\"\"Get table history.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    dt = DeltaTable(path)\n",
    "    return dt.history()\n",
    "\n",
    "def get_schema(table_name: str) -> dict:\n",
    "    \"\"\"Get table schema.\"\"\"\n",
    "    path = get_table_path(table_name)\n",
    "    dt = DeltaTable(path)\n",
    "    return dt.schema().to_pyarrow()\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Check Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Delta Lake Tables:\n",
      "========================================\n",
      "customers       âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:05.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:05.000Z': error parsing date)\n",
      "products        âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date)\n",
      "orders          âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'order_date': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date)\n",
      "order_items     âœ… Available (error: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:07.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:07.000Z': error parsing date)\n",
      "cdc_events      âœ… Available (17 rows, 1 versions)\n"
     ]
    }
   ],
   "source": [
    "# Check which tables exist\n",
    "print(\"Available Delta Lake Tables:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for table in TABLES:\n",
    "    exists = table_exists(table)\n",
    "    status = \"âœ… Available\" if exists else \"âŒ Not found\"\n",
    "    \n",
    "    if exists:\n",
    "        try:\n",
    "            df = load_table(table)\n",
    "            row_count = len(df)\n",
    "            history = get_history(table)\n",
    "            version_count = len(history)\n",
    "            print(f\"{table:15} {status} ({row_count} rows, {version_count} versions)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{table:15} {status} (error: {e})\")\n",
    "    else:\n",
    "        print(f\"{table:15} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Query Current Data (delta-rs)\n",
    "\n",
    "**Method:** Python-native `deltalake` library (delta-rs)  \n",
    "**Pros:** Fast, lightweight, no JVM overhead  \n",
    "**Cons:** Limited to basic operations (read, time travel, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:05.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:05.000Z': error parsing date",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query customers table\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table_exists(\u001b[33m\"\u001b[39m\u001b[33mcustomers\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_customers = \u001b[43mload_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCustomers Table (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_customers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mload_table\u001b[39m\u001b[34m(table_name, version)\u001b[39m\n\u001b[32m     15\u001b[39m     dt = DeltaTable(path, version=version)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     dt = \u001b[43mDeltaTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dt.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/deltalake_poc/lib/python3.14/site-packages/deltalake/table.py:415\u001b[39m, in \u001b[36mDeltaTable.__init__\u001b[39m\u001b[34m(self, table_uri, version, storage_options, without_files, log_buffer_size)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03mCreate the Delta Table from a path with an optional version.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03mMultiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \n\u001b[32m    413\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_options = storage_options\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28mself\u001b[39m._table = \u001b[43mRawDeltaTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mException\u001b[39m: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:05.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:05.000Z': error parsing date"
     ]
    }
   ],
   "source": [
    "# Query customers table\n",
    "if table_exists(\"customers\"):\n",
    "    df_customers = load_table(\"customers\")\n",
    "    print(f\"Customers Table ({len(df_customers)} rows)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_customers)\n",
    "else:\n",
    "    print(\"Customers table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query products table\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table_exists(\u001b[33m\"\u001b[39m\u001b[33mproducts\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_products = \u001b[43mload_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproducts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProducts Table (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_products)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mload_table\u001b[39m\u001b[34m(table_name, version)\u001b[39m\n\u001b[32m     15\u001b[39m     dt = DeltaTable(path, version=version)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     dt = \u001b[43mDeltaTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dt.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/deltalake_poc/lib/python3.14/site-packages/deltalake/table.py:415\u001b[39m, in \u001b[36mDeltaTable.__init__\u001b[39m\u001b[34m(self, table_uri, version, storage_options, without_files, log_buffer_size)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03mCreate the Delta Table from a path with an optional version.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03mMultiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \n\u001b[32m    413\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_options = storage_options\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28mself\u001b[39m._table = \u001b[43mRawDeltaTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwithout_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mException\u001b[39m: Json error: whilst decoding field 'minValues': whilst decoding field 'created_at': failed to parse \"+57947-06-07T01:15:06.000Z\" as Timestamp(Microsecond, Some(\"UTC\")): Parser error: Error parsing timestamp from '+57947-06-07T01:15:06.000Z': error parsing date"
     ]
    }
   ],
   "source": [
    "# Query products table\n",
    "if table_exists(\"products\"):\n",
    "    df_products = load_table(\"products\")\n",
    "    print(f\"Products Table ({len(df_products)} rows)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_products)\n",
    "else:\n",
    "    print(\"Products table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders Table (3 rows)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>status</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>shipping_address</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>pending</td>\n",
       "      <td>259.97</td>\n",
       "      <td>789 Pine Rd, Chicago, IL 60601</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:20+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>shipped</td>\n",
       "      <td>79.99</td>\n",
       "      <td>456 Oak Ave, Los Angeles, CA 90001</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:19+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>1349.98</td>\n",
       "      <td>123 Main St, New York, NY 10001</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:17+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  customer_id                order_date     status  total_amount  \\\n",
       "0   3            3 1970-01-21 10:41:11+00:00    pending        259.97   \n",
       "1   2            2 1970-01-21 10:41:11+00:00    shipped         79.99   \n",
       "2   1            1 1970-01-21 10:41:11+00:00  completed       1349.98   \n",
       "\n",
       "                     shipping_address                created_at  \\\n",
       "0      789 Pine Rd, Chicago, IL 60601 1970-01-21 10:41:11+00:00   \n",
       "1  456 Oak Ave, Los Angeles, CA 90001 1970-01-21 10:41:11+00:00   \n",
       "2     123 Main St, New York, NY 10001 1970-01-21 10:41:11+00:00   \n",
       "\n",
       "                 updated_at __cdc_operation           __cdc_timestamp  \\\n",
       "0 1970-01-21 10:41:11+00:00               r 2025-12-23 06:29:50+00:00   \n",
       "1 1970-01-21 10:41:11+00:00               r 2025-12-23 06:29:50+00:00   \n",
       "2 1970-01-21 10:41:11+00:00               r 2025-12-23 06:29:50+00:00   \n",
       "\n",
       "             __processed_at  \n",
       "0 2025-12-23 06:43:20+00:00  \n",
       "1 2025-12-23 06:43:19+00:00  \n",
       "2 2025-12-23 06:43:17+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query orders table\n",
    "if table_exists(\"orders\"):\n",
    "    df_orders = load_table(\"orders\")\n",
    "    print(f\"Orders Table ({len(df_orders)} rows)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_orders)\n",
    "else:\n",
    "    print(\"Orders table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Events (17 events)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>source_table</th>\n",
       "      <th>operation</th>\n",
       "      <th>record_id</th>\n",
       "      <th>before_data</th>\n",
       "      <th>after_data</th>\n",
       "      <th>kafka_topic</th>\n",
       "      <th>kafka_partition</th>\n",
       "      <th>kafka_offset</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdc.public.products-0-4</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 5, name: Monitor Stand, description: Adju...</td>\n",
       "      <td>cdc.public.products</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-23 06:29:50.604000+00:00</td>\n",
       "      <td>2025-12-23 06:43:26.687228+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdc.public.products-0-3</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 4, name: Mechanical Keyboard, description...</td>\n",
       "      <td>cdc.public.products</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.604000+00:00</td>\n",
       "      <td>2025-12-23 06:43:25.498653+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdc.public.products-0-2</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 3, name: USB-C Hub, description: 7-in-1 U...</td>\n",
       "      <td>cdc.public.products</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "      <td>2025-12-23 06:43:24.396446+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdc.public.products-0-1</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 2, name: Wireless Mouse, description: Erg...</td>\n",
       "      <td>cdc.public.products</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "      <td>2025-12-23 06:43:23.216876+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdc.public.products-0-0</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 1, name: Laptop Pro, description: High-pe...</td>\n",
       "      <td>cdc.public.products</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "      <td>2025-12-23 06:43:21.824763+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cdc.public.orders-0-2</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 3, customer_id: 3, order_date: 1766471296...</td>\n",
       "      <td>cdc.public.orders</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "      <td>2025-12-23 06:43:20.529542+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cdc.public.orders-0-1</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 2, customer_id: 2, order_date: 1766471296...</td>\n",
       "      <td>cdc.public.orders</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "      <td>2025-12-23 06:43:18.864304+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cdc.public.customers-0-4</td>\n",
       "      <td>public.customers</td>\n",
       "      <td>r</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 5, first_name: Charlie, last_name: Brown,...</td>\n",
       "      <td>cdc.public.customers</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-23 06:29:50.598000+00:00</td>\n",
       "      <td>2025-12-23 06:43:09.677926+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cdc.public.customers-0-1</td>\n",
       "      <td>public.customers</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 2, first_name: Jane, last_name: Smith, em...</td>\n",
       "      <td>cdc.public.customers</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.597000+00:00</td>\n",
       "      <td>2025-12-23 06:43:04.313216+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cdc.public.order_items-0-3</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 4, order_id: 3, product_id: 4, quantity: ...</td>\n",
       "      <td>cdc.public.order_items</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.610000+00:00</td>\n",
       "      <td>2025-12-23 06:43:15.237082+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cdc.public.customers-0-0</td>\n",
       "      <td>public.customers</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 1, first_name: John, last_name: Doe, emai...</td>\n",
       "      <td>cdc.public.customers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-23 06:29:50.595000+00:00</td>\n",
       "      <td>2025-12-23 06:42:59.360054+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cdc.public.orders-0-0</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 1, customer_id: 1, order_date: 1766471296...</td>\n",
       "      <td>cdc.public.orders</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "      <td>2025-12-23 06:43:16.497634+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cdc.public.order_items-0-0</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 1, order_id: 1, product_id: 1, quantity: ...</td>\n",
       "      <td>cdc.public.order_items</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-12-23 06:29:50.609000+00:00</td>\n",
       "      <td>2025-12-23 06:43:11.154674+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cdc.public.order_items-0-1</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 2, order_id: 1, product_id: 2, quantity: ...</td>\n",
       "      <td>cdc.public.order_items</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.609000+00:00</td>\n",
       "      <td>2025-12-23 06:43:12.609019+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cdc.public.customers-0-2</td>\n",
       "      <td>public.customers</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 3, first_name: Bob, last_name: Johnson, e...</td>\n",
       "      <td>cdc.public.customers</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.598000+00:00</td>\n",
       "      <td>2025-12-23 06:43:06.322374+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cdc.public.customers-0-3</td>\n",
       "      <td>public.customers</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 4, first_name: Alice, last_name: Williams...</td>\n",
       "      <td>cdc.public.customers</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.598000+00:00</td>\n",
       "      <td>2025-12-23 06:43:07.970395+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cdc.public.order_items-0-2</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>{id: 3, order_id: 2, product_id: 3, quantity: ...</td>\n",
       "      <td>cdc.public.order_items</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.609000+00:00</td>\n",
       "      <td>2025-12-23 06:43:13.877722+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      event_id        source_table operation  record_id  \\\n",
       "0      cdc.public.products-0-4     public.products         r          5   \n",
       "1      cdc.public.products-0-3     public.products         r          4   \n",
       "2      cdc.public.products-0-2     public.products         r          3   \n",
       "3      cdc.public.products-0-1     public.products         r          2   \n",
       "4      cdc.public.products-0-0     public.products         r          1   \n",
       "5        cdc.public.orders-0-2       public.orders         r          3   \n",
       "6        cdc.public.orders-0-1       public.orders         r          2   \n",
       "7     cdc.public.customers-0-4    public.customers         r          5   \n",
       "8     cdc.public.customers-0-1    public.customers         r          2   \n",
       "9   cdc.public.order_items-0-3  public.order_items         r          4   \n",
       "10    cdc.public.customers-0-0    public.customers         r          1   \n",
       "11       cdc.public.orders-0-0       public.orders         r          1   \n",
       "12  cdc.public.order_items-0-0  public.order_items         r          1   \n",
       "13  cdc.public.order_items-0-1  public.order_items         r          2   \n",
       "14    cdc.public.customers-0-2    public.customers         r          3   \n",
       "15    cdc.public.customers-0-3    public.customers         r          4   \n",
       "16  cdc.public.order_items-0-2  public.order_items         r          3   \n",
       "\n",
       "   before_data                                         after_data  \\\n",
       "0         None  {id: 5, name: Monitor Stand, description: Adju...   \n",
       "1         None  {id: 4, name: Mechanical Keyboard, description...   \n",
       "2         None  {id: 3, name: USB-C Hub, description: 7-in-1 U...   \n",
       "3         None  {id: 2, name: Wireless Mouse, description: Erg...   \n",
       "4         None  {id: 1, name: Laptop Pro, description: High-pe...   \n",
       "5         None  {id: 3, customer_id: 3, order_date: 1766471296...   \n",
       "6         None  {id: 2, customer_id: 2, order_date: 1766471296...   \n",
       "7         None  {id: 5, first_name: Charlie, last_name: Brown,...   \n",
       "8         None  {id: 2, first_name: Jane, last_name: Smith, em...   \n",
       "9         None  {id: 4, order_id: 3, product_id: 4, quantity: ...   \n",
       "10        None  {id: 1, first_name: John, last_name: Doe, emai...   \n",
       "11        None  {id: 1, customer_id: 1, order_date: 1766471296...   \n",
       "12        None  {id: 1, order_id: 1, product_id: 1, quantity: ...   \n",
       "13        None  {id: 2, order_id: 1, product_id: 2, quantity: ...   \n",
       "14        None  {id: 3, first_name: Bob, last_name: Johnson, e...   \n",
       "15        None  {id: 4, first_name: Alice, last_name: Williams...   \n",
       "16        None  {id: 3, order_id: 2, product_id: 3, quantity: ...   \n",
       "\n",
       "               kafka_topic  kafka_partition  kafka_offset  \\\n",
       "0      cdc.public.products                0             4   \n",
       "1      cdc.public.products                0             3   \n",
       "2      cdc.public.products                0             2   \n",
       "3      cdc.public.products                0             1   \n",
       "4      cdc.public.products                0             0   \n",
       "5        cdc.public.orders                0             2   \n",
       "6        cdc.public.orders                0             1   \n",
       "7     cdc.public.customers                0             4   \n",
       "8     cdc.public.customers                0             1   \n",
       "9   cdc.public.order_items                0             3   \n",
       "10    cdc.public.customers                0             0   \n",
       "11       cdc.public.orders                0             0   \n",
       "12  cdc.public.order_items                0             0   \n",
       "13  cdc.public.order_items                0             1   \n",
       "14    cdc.public.customers                0             2   \n",
       "15    cdc.public.customers                0             3   \n",
       "16  cdc.public.order_items                0             2   \n",
       "\n",
       "                    event_timestamp                     processed_at  \n",
       "0  2025-12-23 06:29:50.604000+00:00 2025-12-23 06:43:26.687228+00:00  \n",
       "1  2025-12-23 06:29:50.604000+00:00 2025-12-23 06:43:25.498653+00:00  \n",
       "2  2025-12-23 06:29:50.603000+00:00 2025-12-23 06:43:24.396446+00:00  \n",
       "3  2025-12-23 06:29:50.603000+00:00 2025-12-23 06:43:23.216876+00:00  \n",
       "4  2025-12-23 06:29:50.603000+00:00 2025-12-23 06:43:21.824763+00:00  \n",
       "5  2025-12-23 06:29:50.607000+00:00 2025-12-23 06:43:20.529542+00:00  \n",
       "6  2025-12-23 06:29:50.607000+00:00 2025-12-23 06:43:18.864304+00:00  \n",
       "7  2025-12-23 06:29:50.598000+00:00 2025-12-23 06:43:09.677926+00:00  \n",
       "8  2025-12-23 06:29:50.597000+00:00 2025-12-23 06:43:04.313216+00:00  \n",
       "9  2025-12-23 06:29:50.610000+00:00 2025-12-23 06:43:15.237082+00:00  \n",
       "10 2025-12-23 06:29:50.595000+00:00 2025-12-23 06:42:59.360054+00:00  \n",
       "11 2025-12-23 06:29:50.607000+00:00 2025-12-23 06:43:16.497634+00:00  \n",
       "12 2025-12-23 06:29:50.609000+00:00 2025-12-23 06:43:11.154674+00:00  \n",
       "13 2025-12-23 06:29:50.609000+00:00 2025-12-23 06:43:12.609019+00:00  \n",
       "14 2025-12-23 06:29:50.598000+00:00 2025-12-23 06:43:06.322374+00:00  \n",
       "15 2025-12-23 06:29:50.598000+00:00 2025-12-23 06:43:07.970395+00:00  \n",
       "16 2025-12-23 06:29:50.609000+00:00 2025-12-23 06:43:13.877722+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query CDC events (audit log)\n",
    "if table_exists(\"cdc_events\"):\n",
    "    df_events = load_table(\"cdc_events\")\n",
    "    print(f\"CDC Events ({len(df_events)} events)\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df_events.head(20))\n",
    "else:\n",
    "    print(\"CDC events table not found. Run the CDC pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Detailed Version Analysis: customers ===\n",
      "\n",
      "Total versions: 6\n",
      "\n",
      "Version Details:\n",
      "====================================================================================================\n",
      "v 5 | 1766472191050 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 4 | 1766472189552 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 3 | 1766472187860 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 2 | 1766472186208 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 1 | 1766472184122 | MERGE           | Rows: out=1, updated=0, inserted=1\n",
      "v 0 | 1766472180826 | CREATE TABLE    | Rows: out=N/A, updated=N/A, inserted=N/A\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Key Metrics:\n",
      "- 'numOutputRows': Total rows after operation\n",
      "- 'numTargetRowsUpdated': Rows updated by MERGE\n",
      "- 'numTargetRowsInserted': Rows inserted by MERGE\n"
     ]
    }
   ],
   "source": [
    "# Detailed version analysis - see what changed in each version\n",
    "TABLE_TO_ANALYZE = \"customers\"\n",
    "\n",
    "if table_exists(TABLE_TO_ANALYZE):\n",
    "    print(f\"=== Detailed Version Analysis: {TABLE_TO_ANALYZE} ===\\n\")\n",
    "    \n",
    "    history = get_history(TABLE_TO_ANALYZE)\n",
    "    \n",
    "    print(f\"Total versions: {len(history)}\\n\")\n",
    "    print(\"Version Details:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for entry in history[:15]:  # Show last 15 versions\n",
    "        version = entry.get('version', 'N/A')\n",
    "        timestamp = entry.get('timestamp', 'N/A')\n",
    "        operation = entry.get('operation', 'N/A')\n",
    "        \n",
    "        # Get operation metrics if available\n",
    "        metrics = entry.get('operationMetrics', {})\n",
    "        num_output_rows = metrics.get('numOutputRows', 'N/A')\n",
    "        num_updated_rows = metrics.get('numTargetRowsUpdated', 'N/A')\n",
    "        num_inserted_rows = metrics.get('numTargetRowsInserted', 'N/A')\n",
    "        \n",
    "        print(f\"v{version:2} | {timestamp} | {operation:15} | Rows: out={num_output_rows}, updated={num_updated_rows}, inserted={num_inserted_rows}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    print(\"- 'numOutputRows': Total rows after operation\")\n",
    "    print(\"- 'numTargetRowsUpdated': Rows updated by MERGE\")\n",
    "    print(\"- 'numTargetRowsInserted': Rows inserted by MERGE\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_ANALYZE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Row Count Evolution: customers ===\n",
      "\n",
      "Version  5 | MERGE           | 5 rows | 1766472191050\n",
      "Version  4 | MERGE           | 4 rows | 1766472189552\n",
      "Version  3 | MERGE           | 3 rows | 1766472187860\n",
      "Version  2 | MERGE           | 2 rows | 1766472186208\n",
      "Version  1 | MERGE           | 1 rows | 1766472184122\n",
      "Version  0 | CREATE TABLE    | 0 rows | 1766472180826\n",
      "\n",
      "ðŸ’¡ Observation:\n",
      "   - If row count doesn't change between versions, that MERGE was an UPDATE\n",
      "   - If row count increases, that MERGE added a new record\n"
     ]
    }
   ],
   "source": [
    "# Compare row count across versions to see when records were added\n",
    "TABLE_TO_ANALYZE = \"customers\"\n",
    "\n",
    "if table_exists(TABLE_TO_ANALYZE):\n",
    "    print(f\"=== Row Count Evolution: {TABLE_TO_ANALYZE} ===\\n\")\n",
    "    \n",
    "    history = get_history(TABLE_TO_ANALYZE)\n",
    "    \n",
    "    for entry in history[:15]:\n",
    "        version = entry.get('version', 'N/A')\n",
    "        \n",
    "        try:\n",
    "            df = load_table(TABLE_TO_ANALYZE, version=version)\n",
    "            row_count = len(df)\n",
    "            \n",
    "            # Get operation info\n",
    "            operation = entry.get('operation', 'N/A')\n",
    "            timestamp = entry.get('timestamp', 'N/A')\n",
    "            \n",
    "            print(f\"Version {version:2} | {operation:15} | {row_count} rows | {timestamp}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Version {version:2} | Error: {e}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Observation:\")\n",
    "    print(\"   - If row count doesn't change between versions, that MERGE was an UPDATE\")\n",
    "    print(\"   - If row count increases, that MERGE added a new record\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_ANALYZE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'customers' at Version 0\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, first_name, last_name, email, phone, created_at, updated_at, __cdc_operation, __cdc_timestamp, __processed_at]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time travel: Query specific version\n",
    "TABLE_TO_EXPLORE = \"customers\"\n",
    "VERSION = 0 \n",
    "\n",
    "if table_exists(TABLE_TO_EXPLORE):\n",
    "    try:\n",
    "        df_historical = load_table(TABLE_TO_EXPLORE, version=VERSION)\n",
    "        print(f\"'{TABLE_TO_EXPLORE}' at Version {VERSION}\")\n",
    "        print(\"=\" * 60)\n",
    "        display(df_historical)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading version {VERSION}: {e}\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_EXPLORE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0: 0 rows\n",
      "Latest version: 4 rows\n",
      "\n",
      "------------------------------- Version 0 -------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, first_name, last_name, email, phone, created_at, updated_at, __cdc_operation, __cdc_timestamp, __processed_at]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------- Latest -------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Williams</td>\n",
       "      <td>alice.williams@example.com</td>\n",
       "      <td>+1-555-0104</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>bob.johnson@example.com</td>\n",
       "      <td>+1-555-0103</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>+1-555-0102</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>+1-555-0101</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>1970-01-21 10:41:11+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-23 06:29:50+00:00</td>\n",
       "      <td>2025-12-23 06:43:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                       email        phone  \\\n",
       "0   4      Alice  Williams  alice.williams@example.com  +1-555-0104   \n",
       "1   3        Bob   Johnson     bob.johnson@example.com  +1-555-0103   \n",
       "2   2       Jane     Smith      jane.smith@example.com  +1-555-0102   \n",
       "3   1       John       Doe        john.doe@example.com  +1-555-0101   \n",
       "\n",
       "                 created_at                updated_at __cdc_operation  \\\n",
       "0 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "1 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "2 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "3 1970-01-21 10:41:11+00:00 1970-01-21 10:41:11+00:00               r   \n",
       "\n",
       "            __cdc_timestamp            __processed_at  \n",
       "0 2025-12-23 06:29:50+00:00 2025-12-23 06:43:08+00:00  \n",
       "1 2025-12-23 06:29:50+00:00 2025-12-23 06:43:06+00:00  \n",
       "2 2025-12-23 06:29:50+00:00 2025-12-23 06:43:04+00:00  \n",
       "3 2025-12-23 06:29:50+00:00 2025-12-23 06:43:00+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare two versions\n",
    "TABLE_TO_COMPARE = \"customers\"\n",
    "VERSION_OLD = 0\n",
    "VERSION_NEW = 4  # None = latest\n",
    "\n",
    "if table_exists(TABLE_TO_COMPARE):\n",
    "    try:\n",
    "        df_old = load_table(TABLE_TO_COMPARE, version=VERSION_OLD)\n",
    "        df_new = load_table(TABLE_TO_COMPARE, version=VERSION_NEW)\n",
    "        \n",
    "        print(f\"Version {VERSION_OLD}: {len(df_old)} rows\")\n",
    "        print(f\"Latest version: {len(df_new)} rows\")\n",
    "        \n",
    "        # Show side by side if small enough\n",
    "        if len(df_old) <= 10 and len(df_new) <= 10:\n",
    "            print(f\"\\n------------------------------- Version {VERSION_OLD} -------------------------------\")\n",
    "            display(df_old)\n",
    "            print(\"\\n------------------------------- Latest -------------------------------\")\n",
    "            display(df_new)\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing versions: {e}\")\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_COMPARE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Events by Operation Type\n",
      "========================================\n",
      "READ (snapshot)         17 events\n"
     ]
    }
   ],
   "source": [
    "# Analyze CDC events by operation type\n",
    "if table_exists(\"cdc_events\"):\n",
    "    df_events = load_table(\"cdc_events\")\n",
    "    \n",
    "    print(\"CDC Events by Operation Type\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    operation_counts = df_events['operation'].value_counts()\n",
    "    operation_map = {'c': 'CREATE', 'u': 'UPDATE', 'd': 'DELETE', 'r': 'READ (snapshot)'}\n",
    "    \n",
    "    for op, count in operation_counts.items():\n",
    "        op_name = operation_map.get(op, op)\n",
    "        print(f\"{op_name:20} {count:5} events\")\n",
    "else:\n",
    "    print(\"CDC events table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Events by Source Table\n",
      "========================================\n",
      "public.products                    5 events\n",
      "public.customers                   5 events\n",
      "public.order_items                 4 events\n",
      "public.orders                      3 events\n"
     ]
    }
   ],
   "source": [
    "# Analyze CDC events by source table\n",
    "if table_exists(\"cdc_events\"):\n",
    "    df_events = load_table(\"cdc_events\")\n",
    "    \n",
    "    print(\"CDC Events by Source Table\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    table_counts = df_events['source_table'].value_counts()\n",
    "    for table, count in table_counts.items():\n",
    "        print(f\"{table:30} {count:5} events\")\n",
    "else:\n",
    "    print(\"CDC events table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent CDC Events (last 10)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>source_table</th>\n",
       "      <th>operation</th>\n",
       "      <th>record_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdc.public.products-0-4</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-23 06:29:50.604000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdc.public.products-0-3</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-23 06:29:50.604000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdc.public.products-0-2</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdc.public.products-0-1</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdc.public.products-0-0</td>\n",
       "      <td>public.products</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.603000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cdc.public.orders-0-2</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cdc.public.orders-0-1</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cdc.public.orders-0-0</td>\n",
       "      <td>public.orders</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-12-23 06:29:50.607000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cdc.public.order_items-0-3</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-23 06:29:50.610000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cdc.public.order_items-0-2</td>\n",
       "      <td>public.order_items</td>\n",
       "      <td>r</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-12-23 06:29:50.609000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      event_id        source_table operation  record_id  \\\n",
       "0      cdc.public.products-0-4     public.products         r          5   \n",
       "1      cdc.public.products-0-3     public.products         r          4   \n",
       "2      cdc.public.products-0-2     public.products         r          3   \n",
       "3      cdc.public.products-0-1     public.products         r          2   \n",
       "4      cdc.public.products-0-0     public.products         r          1   \n",
       "5        cdc.public.orders-0-2       public.orders         r          3   \n",
       "6        cdc.public.orders-0-1       public.orders         r          2   \n",
       "11       cdc.public.orders-0-0       public.orders         r          1   \n",
       "9   cdc.public.order_items-0-3  public.order_items         r          4   \n",
       "16  cdc.public.order_items-0-2  public.order_items         r          3   \n",
       "\n",
       "                    event_timestamp  \n",
       "0  2025-12-23 06:29:50.604000+00:00  \n",
       "1  2025-12-23 06:29:50.604000+00:00  \n",
       "2  2025-12-23 06:29:50.603000+00:00  \n",
       "3  2025-12-23 06:29:50.603000+00:00  \n",
       "4  2025-12-23 06:29:50.603000+00:00  \n",
       "5  2025-12-23 06:29:50.607000+00:00  \n",
       "6  2025-12-23 06:29:50.607000+00:00  \n",
       "11 2025-12-23 06:29:50.607000+00:00  \n",
       "9  2025-12-23 06:29:50.610000+00:00  \n",
       "16 2025-12-23 06:29:50.609000+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View recent CDC events\n",
    "if table_exists(\"cdc_events\"):\n",
    "    df_events = load_table(\"cdc_events\")\n",
    "    \n",
    "    print(\"Recent CDC Events (last 10)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort by processed_at if available, otherwise by event_id\n",
    "    if 'processed_at' in df_events.columns:\n",
    "        df_recent = df_events.sort_values('processed_at', ascending=False).head(10)\n",
    "    else:\n",
    "        df_recent = df_events.tail(10)\n",
    "    \n",
    "    display(df_recent[['event_id', 'source_table', 'operation', 'record_id', 'event_timestamp']])\n",
    "else:\n",
    "    print(\"CDC events table not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PySpark SQL Queries\n",
    "\n",
    "**Method:** PySpark with Delta Lake SQL  \n",
    "**Pros:** Full SQL support, complex transformations, aggregations  \n",
    "**Cons:** Requires JVM, heavier resource usage  \n",
    "**Python 3.14 Note:** Use SQL-based operations, avoid `createDataFrame()` due to serialization issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created!\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark with Delta Lake\n",
    "USE_SPARK = True  # Set to True to enable PySpark\n",
    "\n",
    "if USE_SPARK:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta import configure_spark_with_delta_pip\n",
    "\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"DeltaLakeNotebook\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    )\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    print(\"SparkSession created!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "else:\n",
    "    print(\"PySpark disabled. Set USE_SPARK = True to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ PySpark SQL Query ------------------------ \n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      John|      Doe|john.doe@example.com|+1-555-0101|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Total customers: 5\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: Query with SQL\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    # Load Delta tables\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    if table_exists(\"customers\"):\n",
    "        # Method 1: Read as DataFrame\n",
    "        df_spark = spark.read.format(\"delta\").load(customers_path)\n",
    "        df_spark.createOrReplaceTempView(\"customers\")\n",
    "        \n",
    "        # Method 2: Run SQL query\n",
    "        print(\"------------------------ PySpark SQL Query ------------------------ \")\n",
    "        result = spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM customers\n",
    "            ORDER BY id\n",
    "        \"\"\")\n",
    "        result.show()\n",
    "        \n",
    "        print(f\"\\nTotal customers: {result.count()}\")\n",
    "    else:\n",
    "        print(\"Customers table not found.\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------  PySpark Time Travel ------------------------\n",
      "\n",
      "Version 0 (Initial Snapshot):\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      John|      Doe|john.doe@example.com|+1-555-0101|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Latest Version:\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      John|      Doe|john.doe@example.com|+1-555-0101|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  6|      Test|     User|test.user@example...|+1-555-9999|+57947-06-12 19:2...|+57947-06-12 19:2...|              c|2025-12-23 15:11:...|2025-12-23 15:11:...|\n",
      "|  6|      Test|     User|updated.test@exam...|+1-555-9999|+57947-06-12 19:2...|+57947-06-12 19:2...|              u|2025-12-23 15:11:...|2025-12-23 15:11:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Version 0 rows: 5\n",
      "Latest rows: 7\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: Time travel query\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    if table_exists(\"customers\"):\n",
    "        print(\"------------------------  PySpark Time Travel ------------------------\\n\")\n",
    "        \n",
    "        # Query version 0 (first snapshot)\n",
    "        print(\"Version 0 (Initial Snapshot):\")\n",
    "        df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(customers_path)\n",
    "        df_v0.show()\n",
    "        \n",
    "        # Query latest version\n",
    "        print(\"\\nLatest Version:\")\n",
    "        df_latest = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(customers_path)\n",
    "        df_latest.show()\n",
    "        \n",
    "        print(f\"\\nVersion 0 rows: {df_v0.count()}\")\n",
    "        print(f\"Latest rows: {df_latest.count()}\")\n",
    "    else:\n",
    "        print(\"Customers table not found.\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Advanced SQL Queries -------------------------------\n",
      "\n",
      "1. Group by CDC operation:\n",
      "+---------------+-----------+\n",
      "|__cdc_operation|event_count|\n",
      "+---------------+-----------+\n",
      "|              r|          5|\n",
      "|              c|          1|\n",
      "|              u|          1|\n",
      "+---------------+-----------+\n",
      "\n",
      "\n",
      "2. Filter and sort customers:\n",
      "+----------+---------+--------------------+-----------+\n",
      "|first_name|last_name|               email|      phone|\n",
      "+----------+---------+--------------------+-----------+\n",
      "|     Alice| Williams|alice.williams@ex...|+1-555-0104|\n",
      "|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|\n",
      "|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|\n",
      "|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|\n",
      "|      John|      Doe|john.doe@example.com|+1-555-0101|\n",
      "+----------+---------+--------------------+-----------+\n",
      "\n",
      "\n",
      "3. Recent changes (by CDC timestamp):\n",
      "+----------+---------+---------------+--------------------+\n",
      "|first_name|last_name|__cdc_operation|     __cdc_timestamp|\n",
      "+----------+---------+---------------+--------------------+\n",
      "|      Test|     User|              u|2025-12-23 15:11:...|\n",
      "|      Test|     User|              c|2025-12-23 15:11:...|\n",
      "|     Alice| Williams|              r|2025-12-23 15:05:...|\n",
      "|   Charlie|    Brown|              r|2025-12-23 15:05:...|\n",
      "|      Jane|    Smith|              r|2025-12-23 15:05:...|\n",
      "+----------+---------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: Advanced aggregations and filtering\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    df = spark.read.format(\"delta\").load(customers_path)\n",
    "    df.createOrReplaceTempView(\"customers\")\n",
    "    \n",
    "    print(\"------------------------------- Advanced SQL Queries -------------------------------\\n\")\n",
    "    \n",
    "    print(\"1. Group by CDC operation:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT __cdc_operation, COUNT(*) as event_count\n",
    "        FROM customers\n",
    "        GROUP BY __cdc_operation\n",
    "        ORDER BY event_count DESC\n",
    "    \"\"\").show()\n",
    "    \n",
    "    print(\"\\n2. Filter and sort customers:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT first_name, last_name, email, phone\n",
    "        FROM customers\n",
    "        ORDER BY first_name, last_name\n",
    "        LIMIT 5\n",
    "    \"\"\").show()\n",
    "    \n",
    "    print(\"\\n3. Recent changes (by CDC timestamp):\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT first_name, last_name, __cdc_operation, __cdc_timestamp\n",
    "        FROM customers\n",
    "        ORDER BY __cdc_timestamp DESC\n",
    "        LIMIT 5\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Delta Table History (Real CDC Data) -------------------------------\n",
      "\n",
      "Showing version history for 'customers' table:\n",
      "\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1      |2025-12-23 15:11:58.029|MERGE    |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3092, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 2888, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2066, numTargetRowsUpdated -> 0, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 131}|\n",
      "|0      |2025-12-23 15:06:29.64 |WRITE    |{numFiles -> 2, numOutputRows -> 5, numOutputBytes -> 4342}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL: View Delta table history (on actual CDC tables)\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    from delta import DeltaTable\n",
    "    \n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    print(\"------------------------------- Delta Table History (Real CDC Data) -------------------------------\\n\")\n",
    "    print(\"Showing version history for 'customers' table:\\n\")\n",
    "    \n",
    "    dt = DeltaTable.forPath(spark, customers_path)\n",
    "    dt.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CDC Events Timeline Analysis ===\n",
      "\n",
      "1. Events by operation type:\n",
      "+---------+-----------+--------------------+--------------------+\n",
      "|operation|event_count|         first_event|          last_event|\n",
      "+---------+-----------+--------------------+--------------------+\n",
      "|        r|         17|2025-12-23 15:05:...|2025-12-23 15:05:...|\n",
      "|        c|          3|2025-12-23 15:11:...|2025-12-23 15:11:...|\n",
      "|        u|          2|2025-12-23 15:11:...|2025-12-23 15:11:...|\n",
      "+---------+-----------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "2. Events by source table:\n",
      "+------------------+---------+-----+\n",
      "|      source_table|operation|count|\n",
      "+------------------+---------+-----+\n",
      "|  public.customers|        r|    5|\n",
      "|  public.customers|        u|    1|\n",
      "|  public.customers|        c|    1|\n",
      "|public.order_items|        r|    4|\n",
      "|     public.orders|        r|    3|\n",
      "|     public.orders|        c|    1|\n",
      "|   public.products|        r|    5|\n",
      "|   public.products|        c|    1|\n",
      "|   public.products|        u|    1|\n",
      "+------------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: CDC Events Analysis\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"cdc_events\"):\n",
    "    events_path = get_table_path(\"cdc_events\")\n",
    "    \n",
    "    df_events = spark.read.format(\"delta\").load(events_path)\n",
    "    df_events.createOrReplaceTempView(\"cdc_events\")\n",
    "    \n",
    "    print(\"=== CDC Events Timeline Analysis ===\\n\")\n",
    "    \n",
    "    print(\"1. Events by operation type:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            operation,\n",
    "            COUNT(*) as event_count,\n",
    "            MIN(event_timestamp) as first_event,\n",
    "            MAX(event_timestamp) as last_event\n",
    "        FROM cdc_events\n",
    "        GROUP BY operation\n",
    "        ORDER BY event_count DESC\n",
    "    \"\"\").show()\n",
    "    \n",
    "    print(\"\\n2. Events by source table:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            source_table,\n",
    "            operation,\n",
    "            COUNT(*) as count\n",
    "        FROM cdc_events\n",
    "        GROUP BY source_table, operation\n",
    "        ORDER BY source_table, count DESC\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or cdc_events table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time Travel: Compare Customer Versions ===\n",
      "\n",
      "Available versions: 0 to 1\n",
      "\n",
      "----------------------------- Version 0 (Initial) -----------------------------\n",
      "Row count: 5\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "| id|first_name|last_name|               email|      phone|          created_at|          updated_at|__cdc_operation|     __cdc_timestamp|      __processed_at|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|  1|      John|      Doe|john.doe@example.com|+1-555-0101|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  2|      Jane|    Smith|jane.smith@exampl...|+1-555-0102|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  3|       Bob|  Johnson|bob.johnson@examp...|+1-555-0103|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  4|     Alice| Williams|alice.williams@ex...|+1-555-0104|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "|  5|   Charlie|    Brown|charlie.brown@exa...|+1-555-0105|+57947-06-07 08:1...|+57947-06-07 08:1...|              r|2025-12-23 15:05:...|2025-12-23 15:06:...|\n",
      "+---+----------+---------+--------------------+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "----------------------------- Latest Version (v1) -----------------------------\n",
      "Error: Cannot time travel Delta table to version 6. Available versions: [0, 1].\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Compare versions of customers table (Time Travel)\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    print(\"=== Time Travel: Compare Customer Versions ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get version count\n",
    "        from delta import DeltaTable\n",
    "        dt = DeltaTable.forPath(spark, customers_path)\n",
    "        history = dt.history().select(\"version\").collect()\n",
    "        max_version = max([row.version for row in history])\n",
    "        \n",
    "        print(f\"Available versions: 0 to {max_version}\\n\")\n",
    "        \n",
    "        # Compare version 0 vs latest\n",
    "        print(\"----------------------------- Version 0 (Initial) -----------------------------\")\n",
    "        df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(customers_path)\n",
    "        print(f\"Row count: {df_v0.count()}\")\n",
    "        df_v0.show(5)\n",
    "        \n",
    "        print(f\"\\n----------------------------- Latest Version (v{max_version}) -----------------------------\")\n",
    "        df_v6 = spark.read.format(\"delta\").option(\"versionAsOf\", 6).load(customers_path)\n",
    "        print(f\"Row count: {df_v0.count()}\")\n",
    "        df_v6.show(5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Time Travel by Timestamp: Version 5 vs Version 6 ===\n",
      "\n",
      "Version 0 timestamp: 2025-12-23 15:06:29.640000\n",
      "Version 1 timestamp: 2025-12-23 15:11:58.029000\n",
      "\n",
      "----------------------------- Version 0 -----------------------------\n",
      "Row count: 5\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|id |first_name|last_name|email                     |phone      |created_at           |updated_at           |__cdc_operation|__cdc_timestamp        |__processed_at         |\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|1  |John      |Doe      |john.doe@example.com      |+1-555-0101|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.156|2025-12-23 15:06:21.757|\n",
      "|2  |Jane      |Smith    |jane.smith@example.com    |+1-555-0102|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.158|2025-12-23 15:06:21.757|\n",
      "|3  |Bob       |Johnson  |bob.johnson@example.com   |+1-555-0103|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.158|2025-12-23 15:06:21.757|\n",
      "|4  |Alice     |Williams |alice.williams@example.com|+1-555-0104|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.159|2025-12-23 15:06:21.757|\n",
      "|5  |Charlie   |Brown    |charlie.brown@example.com |+1-555-0105|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.159|2025-12-23 15:06:21.757|\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "\n",
      "----------------------------- Version 1 -----------------------------\n",
      "Row count: 7\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|id |first_name|last_name|email                     |phone      |created_at           |updated_at           |__cdc_operation|__cdc_timestamp        |__processed_at         |\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "|1  |John      |Doe      |john.doe@example.com      |+1-555-0101|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.156|2025-12-23 15:06:21.757|\n",
      "|2  |Jane      |Smith    |jane.smith@example.com    |+1-555-0102|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.158|2025-12-23 15:06:21.757|\n",
      "|3  |Bob       |Johnson  |bob.johnson@example.com   |+1-555-0103|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.158|2025-12-23 15:06:21.757|\n",
      "|4  |Alice     |Williams |alice.williams@example.com|+1-555-0104|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.159|2025-12-23 15:06:21.757|\n",
      "|5  |Charlie   |Brown    |charlie.brown@example.com |+1-555-0105|+57947-06-07 08:15:05|+57947-06-07 08:15:05|r              |2025-12-23 15:05:29.159|2025-12-23 15:06:21.757|\n",
      "+---+----------+---------+--------------------------+-----------+---------------------+---------------------+---------------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "customers_path = get_table_path(\"customers\")\n",
    "\n",
    "print(\"=== Time Travel by Timestamp: Version 5 vs Version 6 ===\\n\")\n",
    "\n",
    "# Load Delta table\n",
    "dt = DeltaTable.forPath(spark, customers_path)\n",
    "\n",
    "# Get history with timestamps\n",
    "history_df = (\n",
    "    dt.history(10)\n",
    "      .select(\"version\", \"timestamp\", \"operation\")\n",
    "      .orderBy(\"version\")\n",
    ")\n",
    "\n",
    "# history_df.show(truncate=False)\n",
    "\n",
    "# Extract timestamps for v5 and v6\n",
    "history = history_df.collect()\n",
    "\n",
    "ts_v5 = next(row.timestamp for row in history if row.version == 0)\n",
    "ts_v6 = next(row.timestamp for row in history if row.version == 1)\n",
    "\n",
    "print(f\"Version 0 timestamp: {ts_v5}\")\n",
    "print(f\"Version 1 timestamp: {ts_v6}\\n\")\n",
    "\n",
    "# Read data AS OF version 0 (by timestamp)\n",
    "df_v5 = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"timestampAsOf\", ts_v5)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# Read data AS OF version 6 (by timestamp)\n",
    "df_v6 = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"timestampAsOf\", ts_v6)\n",
    "    .load(customers_path)\n",
    ")\n",
    "\n",
    "# Compare\n",
    "print(\"----------------------------- Version 0 -----------------------------\")\n",
    "print(f\"Row count: {df_v5.count()}\")\n",
    "df_v5.show(5, truncate=False)\n",
    "\n",
    "print(\"----------------------------- Version 1 -----------------------------\")\n",
    "print(f\"Row count: {df_v6.count()}\")\n",
    "df_v6.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+------------+---------------+----------------------+\n",
      "|id |first_name|last_name|email               |_change_type|_commit_version|_commit_timestamp     |\n",
      "+---+----------+---------+--------------------+------------+---------------+----------------------+\n",
      "|1  |John      |Doe      |john.doe@example.com|insert      |0              |2025-12-23 15:06:29.64|\n",
      "+---+----------+---------+--------------------+------------+---------------+----------------------+\n",
      "\n",
      "\n",
      "Total changes tracked: 1\n"
     ]
    }
   ],
   "source": [
    "# Track all changes on a specific customer record using Change Data Feed\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    from delta import DeltaTable\n",
    "    \n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    try:\n",
    "        # Enable Change Data Feed on the table (if not already enabled)\n",
    "        dt = DeltaTable.forPath(spark, customers_path)\n",
    "        \n",
    "        # all changes (insert, update, delete) for each record\n",
    "        changes_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .load(customers_path)\n",
    "        )\n",
    "        \n",
    "        # changes on a specific customer (ID = 1)\n",
    "        customer_id = 1\n",
    "        customer_changes = changes_df.filter(f\"id = {customer_id}\").orderBy(\"_commit_version\")\n",
    "        \n",
    "        # Show key columns including change metadata\n",
    "        customer_changes.select(\n",
    "            \"id\", \n",
    "            \"first_name\", \n",
    "            \"last_name\", \n",
    "            \"email\",\n",
    "            \"_change_type\",      # insert, update_preimage, update_postimage, delete\n",
    "            \"_commit_version\",   # Delta version number\n",
    "            \"_commit_timestamp\"  # When change occurred\n",
    "        ).show(truncate=False)\n",
    "        \n",
    "        print(f\"\\nTotal changes tracked: {customer_changes.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID 1 - Final States Only (cleaner view):\n",
      "====================================================================================================\n",
      "+---------------+------------+---+----------+---------+--------------------+----------------------+\n",
      "|_commit_version|_change_type|id |first_name|last_name|email               |_commit_timestamp     |\n",
      "+---------------+------------+---+----------+---------+--------------------+----------------------+\n",
      "|0              |insert      |1  |John      |Doe      |john.doe@example.com|2025-12-23 15:06:29.64|\n",
      "+---------------+------------+---+----------+---------+--------------------+----------------------+\n",
      "\n",
      "\n",
      "âœ“ This shows only the 'after' state of each change\n",
      "  Total state changes: 1\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\"):\n",
    "    \n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    \n",
    "    try:\n",
    "        changes_df = (\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .load(customers_path)\n",
    "        )\n",
    "        \n",
    "        customer_id = 1\n",
    "        \n",
    "        final_states = changes_df.filter(\n",
    "            f\"id = {customer_id} AND _change_type IN ('insert', 'update_postimage', 'delete')\"\n",
    "        ).orderBy(\"_commit_version\")\n",
    "        \n",
    "        print(f\"Customer ID {customer_id} - Final States Only (cleaner view):\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        final_states.select(\n",
    "            \"_commit_version\",\n",
    "            \"_change_type\",\n",
    "            \"id\", \n",
    "            \"first_name\", \n",
    "            \"last_name\", \n",
    "            \"email\",\n",
    "            \"_commit_timestamp\"\n",
    "        ).show(truncate=False)\n",
    "        \n",
    "        print(f\"\\nâœ“ This shows only the 'after' state of each change\")\n",
    "        print(f\"  Total state changes: {final_states.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled or customers table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-12-23 15:11:58.029|NULL  |NULL    |MERGE    |{predicate -> [\"(id#5370L = id#4704L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 3092, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 2888, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2066, numTargetRowsUpdated -> 0, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 131}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.2.0|\n",
      "|0      |2025-12-23 15:06:29.64 |NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 2, numOutputRows -> 5, numOutputBytes -> 4342}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.2.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forPath(spark, customers_path)\n",
    "history_df = dt.history()\n",
    "\n",
    "history_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Product Popularity: Most Ordered Items -----------------------------\n",
      "\n",
      "+-------------------+-----------+-------------+-------------------+\n",
      "|       product_name|   category|times_ordered|total_quantity_sold|\n",
      "+-------------------+-----------+-------------+-------------------+\n",
      "|Mechanical Keyboard|Electronics|            1|                  2|\n",
      "|          USB-C Hub|Electronics|            1|                  1|\n",
      "|         Laptop Pro|Electronics|            1|                  1|\n",
      "|     Wireless Mouse|Electronics|            1|                  1|\n",
      "|      Monitor Stand|     Office|            0|               NULL|\n",
      "|       Test Product|       Test|            0|               NULL|\n",
      "+-------------------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Product analysis with order items\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"products\") and table_exists(\"order_items\"):\n",
    "    products_path = get_table_path(\"products\")\n",
    "    items_path = get_table_path(\"order_items\")\n",
    "    \n",
    "    df_products = spark.read.format(\"delta\").load(products_path)\n",
    "    df_items = spark.read.format(\"delta\").load(items_path)\n",
    "    \n",
    "    df_products.createOrReplaceTempView(\"products\")\n",
    "    df_items.createOrReplaceTempView(\"order_items\")\n",
    "    \n",
    "    print(\"----------------------------- Product Popularity: Most Ordered Items -----------------------------\\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.name as product_name,\n",
    "            p.category,\n",
    "            COUNT(oi.id) as times_ordered,\n",
    "            SUM(oi.quantity) as total_quantity_sold\n",
    "        FROM products p\n",
    "        LEFT JOIN order_items oi ON p.id = oi.product_id\n",
    "        GROUP BY p.id, p.name, p.category\n",
    "        ORDER BY total_quantity_sold DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or tables not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Order Analysis: Items per Order -----------------------------\n",
      "\n",
      "+--------+-----------+-------------------+----------+--------------+\n",
      "|order_id|customer_id|         order_date|item_count|total_quantity|\n",
      "+--------+-----------+-------------------+----------+--------------+\n",
      "|       1|          1|1970-01-21 18:41:11|         2|             2|\n",
      "|       3|          3|1970-01-21 18:41:11|         1|             2|\n",
      "|       2|          2|1970-01-21 18:41:11|         1|             1|\n",
      "+--------+-----------+-------------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Analyze order details\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"orders\") and table_exists(\"order_items\"):\n",
    "    orders_path = get_table_path(\"orders\")\n",
    "    items_path = get_table_path(\"order_items\")\n",
    "    \n",
    "    df_orders = spark.read.format(\"delta\").load(orders_path)\n",
    "    df_items = spark.read.format(\"delta\").load(items_path)\n",
    "    \n",
    "    df_orders.createOrReplaceTempView(\"orders\")\n",
    "    df_items.createOrReplaceTempView(\"order_items\")\n",
    "    \n",
    "    print(\"----------------------------- Order Analysis: Items per Order -----------------------------\\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            o.id as order_id,\n",
    "            o.customer_id,\n",
    "            o.order_date,\n",
    "            COUNT(oi.id) as item_count,\n",
    "            SUM(oi.quantity) as total_quantity\n",
    "        FROM orders o\n",
    "        LEFT JOIN order_items oi ON o.id = oi.order_id\n",
    "        GROUP BY o.id, o.customer_id, o.order_date\n",
    "        ORDER BY total_quantity DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or tables not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JOIN: Customers with Their Orders ===\n",
      "\n",
      "+-----------+----------+---------+--------------------+------------+\n",
      "|customer_id|first_name|last_name|               email|total_orders|\n",
      "+-----------+----------+---------+--------------------+------------+\n",
      "|          3|       Bob|  Johnson|bob.johnson@examp...|           1|\n",
      "|          2|      Jane|    Smith|jane.smith@exampl...|           1|\n",
      "|          1|      John|      Doe|john.doe@example.com|           1|\n",
      "|          4|     Alice| Williams|alice.williams@ex...|           0|\n",
      "|          5|   Charlie|    Brown|charlie.brown@exa...|           0|\n",
      "+-----------+----------+---------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark: Join customers with orders\n",
    "if USE_SPARK and 'spark' in dir() and table_exists(\"customers\") and table_exists(\"orders\"):\n",
    "    # Load both tables\n",
    "    customers_path = get_table_path(\"customers\")\n",
    "    orders_path = get_table_path(\"orders\")\n",
    "    \n",
    "    df_customers = spark.read.format(\"delta\").load(customers_path)\n",
    "    df_orders = spark.read.format(\"delta\").load(orders_path)\n",
    "    \n",
    "    df_customers.createOrReplaceTempView(\"customers\")\n",
    "    df_orders.createOrReplaceTempView(\"orders\")\n",
    "    \n",
    "    print(\"=== JOIN: Customers with Their Orders ===\\n\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            c.id as customer_id,\n",
    "            c.first_name,\n",
    "            c.last_name,\n",
    "            c.email,\n",
    "            COUNT(o.id) as total_orders\n",
    "        FROM customers c\n",
    "        LEFT JOIN orders o ON c.id = o.customer_id\n",
    "        GROUP BY c.id, c.first_name, c.last_name, c.email\n",
    "        ORDER BY total_orders DESC\n",
    "    \"\"\").show()\n",
    "else:\n",
    "    print(\"Spark not enabled or tables not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo: Create Table and Insert Records ===\n",
      "\n",
      "Step 1: Creating new Delta table...\n",
      "Table created!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo: Create a new Delta table\n",
    "\n",
    "demo_table_path = \"/Users/anh.nguyen/Documents/poc/deltalake_poc/deltalake/demo_employees\"\n",
    "demo_table_name = \"demo_employees\"\n",
    "\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    print(\"=== Demo: Create Table and Insert Records ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Drop table if exist\n",
    "        spark.sql(f\"\"\"\n",
    "                DROP TABLE IF EXISTS {demo_table_name}\n",
    "                  \"\"\")    \n",
    "\n",
    "        # 1: Create new Delta table\n",
    "        print(\"Step 1: Creating new Delta table...\")\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {demo_table_name} (\n",
    "                id BIGINT NOT NULL,\n",
    "                name STRING NOT NULL,\n",
    "                department STRING,\n",
    "                salary DECIMAL(10, 2),\n",
    "                hire_date DATE,\n",
    "                is_active BOOLEAN\n",
    "            ) USING DELTA\n",
    "            LOCATION '{demo_table_path}'\n",
    "        \"\"\")\n",
    "        print(\"Table created!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Inserting initial records...\n",
      "3 records inserted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo: Insert records using PySpark SQL\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "        \n",
    "    try:        \n",
    "        # 2: Insert initial records\n",
    "        print(\"2: Inserting initial records...\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {demo_table_name} VALUES\n",
    "            (1, 'Alice Johnson', 'Engineering', 95000.00, DATE '2020-01-15', true),\n",
    "            (2, 'Bob Smith', 'Marketing', 75000.00, DATE '2021-03-20', true),\n",
    "            (3, 'Charlie Brown', 'Engineering', 88000.00, DATE '2019-07-10', true)\n",
    "        \"\"\")\n",
    "        print(\"3 records inserted!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Viewing inserted data:\n",
      "+---+-------------+------------+---------+----------+---------+\n",
      "| id|         name|  department|   salary| hire_date|is_active|\n",
      "+---+-------------+------------+---------+----------+---------+\n",
      "|  3|Charlie Brown| Engineering| 88000.00|2019-07-10|     true|\n",
      "|  1|Alice Johnson| Engineering|105000.00|2020-01-15|     true|\n",
      "|  1|Alice Johnson| Engineering| 95000.00|2020-01-15|     true|\n",
      "|  5| Eve Anderson| Engineering| 92000.00|2021-11-08|     true|\n",
      "|  3|Charlie Brown|Data Science| 98000.00|2019-07-10|     true|\n",
      "|  6| Frank Miller|       Sales| 78000.00|2023-01-15|     true|\n",
      "|  2|    Bob Smith|   Marketing| 75000.00|2021-03-20|     true|\n",
      "|  2|    Bob Smith|   Marketing| 75000.00|2021-03-20|    false|\n",
      "|  4| Diana Prince|       Sales| 82000.00|2022-05-12|     true|\n",
      "+---+-------------+------------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "\n",
    "    try:                \n",
    "        # 3: View the data\n",
    "        print(\"Step 3: Viewing inserted data:\")\n",
    "        spark.sql(f\"SELECT * FROM {demo_table_name}\").show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: Inserting additional records...\n",
      "2 more records inserted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:        \n",
    "        # 4: Insert more records\n",
    "        print(\"\\n4: Inserting additional records...\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {demo_table_name} VALUES\n",
    "            (4, 'Diana Prince', 'Sales', 82000.00, DATE '2022-05-12', true),\n",
    "            (5, 'Eve Anderson', 'Engineering', 92000.00, DATE '2021-11-08', true)\n",
    "        \"\"\")\n",
    "        print(\"2 more records inserted!\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Viewing all records:\n",
      "Total records: 5\n",
      "+---+-------------+-----------+--------+----------+---------+\n",
      "| id|         name| department|  salary| hire_date|is_active|\n",
      "+---+-------------+-----------+--------+----------+---------+\n",
      "|  1|Alice Johnson|Engineering|95000.00|2020-01-15|     true|\n",
      "|  2|    Bob Smith|  Marketing|75000.00|2021-03-20|     true|\n",
      "|  3|Charlie Brown|Engineering|88000.00|2019-07-10|     true|\n",
      "|  4| Diana Prince|      Sales|82000.00|2022-05-12|     true|\n",
      "|  5| Eve Anderson|Engineering|92000.00|2021-11-08|     true|\n",
      "+---+-------------+-----------+--------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:        \n",
    "        # 5: View updated data\n",
    "        print(\"Step 5: Viewing all records:\")\n",
    "        result = spark.sql(f\"SELECT * FROM {demo_table_name} ORDER BY id\")\n",
    "        print(f\"Total records: {result.count()}\")\n",
    "        result.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Aggregation - Average salary by department:\n",
      "+-----------+--------------+------------+-------------+\n",
      "| department|employee_count|  avg_salary|earliest_hire|\n",
      "+-----------+--------------+------------+-------------+\n",
      "|Engineering|             3|91666.666667|   2019-07-10|\n",
      "|      Sales|             1|82000.000000|   2022-05-12|\n",
      "|  Marketing|             1|75000.000000|   2021-03-20|\n",
      "+-----------+--------------+------------+-------------+\n",
      "\n",
      "\n",
      "Demo complete! Table created at: /Users/anh.nguyen/Documents/poc/deltalake_poc/deltalake/demo_employees\n"
     ]
    }
   ],
   "source": [
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    try:        \n",
    "        # 6: Perform aggregation\n",
    "        print(\"\\nStep 6: Aggregation - Average salary by department:\")\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                department,\n",
    "                COUNT(*) as employee_count,\n",
    "                AVG(salary) as avg_salary,\n",
    "                MIN(hire_date) as earliest_hire\n",
    "            FROM {demo_table_name}\n",
    "            WHERE is_active = true\n",
    "            GROUP BY department\n",
    "            ORDER BY avg_salary DESC\n",
    "        \"\"\").show()\n",
    "        \n",
    "        print(\"\\nDemo complete! Table created at:\", demo_table_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Spark not enabled. Set USE_SPARK = True in Section 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo: MERGE (Upsert) Operation ===\n",
      "\n",
      "Step 1: Performing MERGE operation...\n",
      "  - Update Charlie's department and salary\n",
      "  - Insert new employee Frank\n",
      "\n",
      "âœ… MERGE complete!\n",
      "\n",
      "ðŸ“ Changes:\n",
      "   - Charlie: department 'Engineering' â†’ 'Data Science', salary $88,000 â†’ $98,000\n",
      "   - Frank: NEW employee added\n",
      "\n",
      "Updated table:\n",
      "+---+-------------+------------+--------+----------+---------+\n",
      "| id|         name|  department|  salary| hire_date|is_active|\n",
      "+---+-------------+------------+--------+----------+---------+\n",
      "|  1|Alice Johnson| Engineering|95000.00|2020-01-15|     true|\n",
      "|  2|    Bob Smith|   Marketing|75000.00|2021-03-20|     true|\n",
      "|  3|Charlie Brown|Data Science|98000.00|2019-07-10|     true|\n",
      "|  4| Diana Prince|       Sales|82000.00|2022-05-12|     true|\n",
      "|  5| Eve Anderson| Engineering|92000.00|2021-11-08|     true|\n",
      "|  6| Frank Miller|       Sales|78000.00|2023-01-15|     true|\n",
      "+---+-------------+------------+--------+----------+---------+\n",
      "\n",
      "\n",
      "Total employees: 6\n"
     ]
    }
   ],
   "source": [
    "# Demo: MERGE (Upsert) operation\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "\n",
    "    if os.path.exists(demo_table_path):\n",
    "        print(\"=== Demo: MERGE (Upsert) Operation ===\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Load table\n",
    "            df = spark.read.format(\"delta\").load(demo_table_path)\n",
    "            df.createOrReplaceTempView(demo_table_name)\n",
    "            \n",
    "            print(\"1: Performing MERGE operation...\")\n",
    "            print(\"  - Update Charlie's department and salary\")\n",
    "            print(\"  - Insert new employee Frank\\n\")\n",
    "            \n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {demo_table_name} AS target\n",
    "                USING (\n",
    "                    SELECT 3 as id, 'Charlie Brown' as name, 'Data Science' as department, \n",
    "                           98000.00 as salary, DATE '2019-07-10' as hire_date, true as is_active\n",
    "                    UNION ALL\n",
    "                    SELECT 6 as id, 'Frank Miller' as name, 'Sales' as department,\n",
    "                           78000.00 as salary, DATE '2023-01-15' as hire_date, true as is_active\n",
    "                ) AS source\n",
    "                ON target.id = source.id\n",
    "                WHEN MATCHED THEN UPDATE SET\n",
    "                    target.department = source.department,\n",
    "                    target.salary = source.salary\n",
    "                WHEN NOT MATCHED THEN INSERT (id, name, department, salary, hire_date, is_active)\n",
    "                    VALUES (source.id, source.name, source.department, source.salary, source.hire_date, source.is_active)\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"âœ… MERGE complete!\\n\")\n",
    "            print(\"Changes:\")\n",
    "            print(\"   - Charlie: department 'Engineering' â†’ 'Data Science', salary $88,000 â†’ $98,000\")\n",
    "            print(\"   - Frank: NEW employee added\\n\")\n",
    "            \n",
    "            # View updated data\n",
    "            print(\"Updated table:\")\n",
    "            df_merged = spark.read.format(\"delta\").load(demo_table_path)\n",
    "            df_merged.orderBy(\"id\").show()\n",
    "            \n",
    "            print(f\"\\nTotal employees: {df_merged.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(\"Demo table not found. Run the 'Create Table' demo first.\")\n",
    "else:\n",
    "    print(\"Spark not enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo: UPDATE and DELETE Operations ===\n",
      "\n",
      "Step 1: Updating Alice's salary...\n",
      "âœ… Salary updated!\n",
      "\n",
      "Step 2: Viewing updated record:\n",
      "+---+-------------+-----------+---------+\n",
      "| id|         name| department|   salary|\n",
      "+---+-------------+-----------+---------+\n",
      "|  1|Alice Johnson|Engineering|105000.00|\n",
      "+---+-------------+-----------+---------+\n",
      "\n",
      "\n",
      "Step 3: Deactivating Bob (soft delete)...\n",
      "âœ… Record deactivated!\n",
      "\n",
      "\n",
      "Step 5: View Delta table history:\n",
      "+-------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|operation   |operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|7      |UPDATE      |{numRemovedFiles -> 1, numRemovedBytes -> 1771, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 622, numDeletionVectorsUpdated -> 0, scanTimeMs -> 478, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1771, rewriteTimeMs -> 144}                                                                                                                                                                                                                                                                                                                          |\n",
      "|6      |UPDATE      |{numRemovedFiles -> 1, numRemovedBytes -> 1813, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 901, numDeletionVectorsUpdated -> 0, scanTimeMs -> 722, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1813, rewriteTimeMs -> 179}                                                                                                                                                                                                                                                                                                                          |\n",
      "|5      |UPDATE      |{numRemovedFiles -> 1, numRemovedBytes -> 1771, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 642, numDeletionVectorsUpdated -> 0, scanTimeMs -> 473, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1771, rewriteTimeMs -> 169}                                                                                                                                                                                                                                                                                                                          |\n",
      "|4      |UPDATE      |{numRemovedFiles -> 1, numRemovedBytes -> 1813, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 895, numDeletionVectorsUpdated -> 0, scanTimeMs -> 710, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1813, rewriteTimeMs -> 182}                                                                                                                                                                                                                                                                                                                          |\n",
      "|3      |MERGE       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1773, numTargetBytesRemoved -> 1813, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 1287, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 788, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 491}|\n",
      "|2      |WRITE       |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 3570}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|1      |WRITE       |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 5397}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|0      |CREATE TABLE|{}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-------+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo: UPDATE and DELETE operations\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    \n",
    "    if os.path.exists(demo_table_path):\n",
    "        print(\"=== Demo: UPDATE and DELETE Operations ===\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Load table\n",
    "            df = spark.read.format(\"delta\").load(demo_table_path)\n",
    "            df.createOrReplaceTempView(demo_table_name)\n",
    "            \n",
    "            # Step 1: UPDATE records\n",
    "            print(\"Step 1: Updating Alice's salary...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE {demo_table_name}\n",
    "                SET salary = 105000.00\n",
    "                WHERE name = 'Alice Johnson'\n",
    "            \"\"\")\n",
    "            print(\"âœ… Salary updated!\\n\")\n",
    "            \n",
    "            # Step 2: View updated data\n",
    "            print(\"Step 2: Viewing updated record:\")\n",
    "            spark.sql(f\"\"\"\n",
    "                SELECT id, name, department, salary\n",
    "                FROM {demo_table_name}\n",
    "                WHERE name = 'Alice Johnson'\n",
    "            \"\"\").show()\n",
    "            \n",
    "            # Step 3: DELETE a record\n",
    "            print(\"\\nStep 3: Deactivating Bob (soft delete)...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE {demo_table_name}\n",
    "                SET is_active = false\n",
    "                WHERE name = 'Bob Smith'\n",
    "            \"\"\")\n",
    "            print(\"âœ… Record deactivated!\\n\")\n",
    "            \n",
    "            # Step 4: View active records only\n",
    "            # print(\"Step 4: Viewing active employees only:\")\n",
    "            # spark.sql(f\"\"\"\n",
    "            #     SELECT id, name, department, salary, is_active\n",
    "            #     FROM {demo_table_name}\n",
    "            #     WHERE is_active = true\n",
    "            #     ORDER BY id\n",
    "            # \"\"\").show()\n",
    "            \n",
    "            # Step 5: View table history\n",
    "            print(\"\\nStep 5: View Delta table history:\")\n",
    "            from delta import DeltaTable\n",
    "            dt = DeltaTable.forPath(spark, demo_table_path)\n",
    "            dt.history().select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(\"Demo table not found. Run the 'Create Table' demo first.\")\n",
    "else:\n",
    "    print(\"Spark not enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark: Stop session when done\n",
    "if USE_SPARK and 'spark' in dir():\n",
    "    # Uncomment to stop Spark session\n",
    "    # spark.stop()\n",
    "    # print(\"SparkSession stopped.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for 'customers' table\n",
      "============================================================\n",
      "id: int64\n",
      "first_name: string\n",
      "last_name: string\n",
      "email: string\n",
      "phone: string\n",
      "created_at: timestamp[us, tz=UTC]\n",
      "updated_at: timestamp[us, tz=UTC]\n",
      "__cdc_operation: string\n",
      "__cdc_timestamp: timestamp[us, tz=UTC]\n",
      "__processed_at: timestamp[us, tz=UTC]\n"
     ]
    }
   ],
   "source": [
    "# View schema for a table\n",
    "TABLE_TO_INSPECT = \"customers\"  # Change to inspect different tables\n",
    "\n",
    "if table_exists(TABLE_TO_INSPECT):\n",
    "    schema = get_schema(TABLE_TO_INSPECT)\n",
    "    print(f\"Schema for '{TABLE_TO_INSPECT}' table\")\n",
    "    print(\"=\" * 60)\n",
    "    print(schema)\n",
    "else:\n",
    "    print(f\"Table '{TABLE_TO_INSPECT}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Table Schemas\n",
      "============================================================\n",
      "\n",
      "--- customers ---\n",
      "  id: int64\n",
      "  first_name: string\n",
      "  last_name: string\n",
      "  email: string\n",
      "  phone: string\n",
      "  created_at: timestamp[us, tz=UTC]\n",
      "  updated_at: timestamp[us, tz=UTC]\n",
      "  __cdc_operation: string\n",
      "  __cdc_timestamp: timestamp[us, tz=UTC]\n",
      "  __processed_at: timestamp[us, tz=UTC]\n",
      "\n",
      "--- products ---\n",
      "  id: int64\n",
      "  name: string\n",
      "  description: string\n",
      "  price: double\n",
      "  stock_quantity: int64\n",
      "  category: string\n",
      "  created_at: timestamp[us, tz=UTC]\n",
      "  updated_at: timestamp[us, tz=UTC]\n",
      "  __cdc_operation: string\n",
      "  __cdc_timestamp: timestamp[us, tz=UTC]\n",
      "  __processed_at: timestamp[us, tz=UTC]\n",
      "\n",
      "--- orders ---\n",
      "  id: int64\n",
      "  customer_id: int64\n",
      "  order_date: timestamp[us, tz=UTC]\n",
      "  status: string\n",
      "  total_amount: double\n",
      "  shipping_address: string\n",
      "  created_at: timestamp[us, tz=UTC]\n",
      "  updated_at: timestamp[us, tz=UTC]\n",
      "  __cdc_operation: string\n",
      "  __cdc_timestamp: timestamp[us, tz=UTC]\n",
      "  __processed_at: timestamp[us, tz=UTC]\n",
      "\n",
      "--- order_items ---\n",
      "  id: int64\n",
      "  order_id: int64\n",
      "  product_id: int64\n",
      "  quantity: int64\n",
      "  unit_price: double\n",
      "  created_at: timestamp[us, tz=UTC]\n",
      "  __cdc_operation: string\n",
      "  __cdc_timestamp: timestamp[us, tz=UTC]\n",
      "  __processed_at: timestamp[us, tz=UTC]\n",
      "\n",
      "--- cdc_events ---\n",
      "  event_id: string\n",
      "  source_table: string\n",
      "  operation: string\n",
      "  record_id: int64\n",
      "  before_data: string\n",
      "  after_data: string\n",
      "  kafka_topic: string\n",
      "  kafka_partition: int32\n",
      "  kafka_offset: int64\n",
      "  event_timestamp: timestamp[us, tz=UTC]\n",
      "  processed_at: timestamp[us, tz=UTC]\n"
     ]
    }
   ],
   "source": [
    "# View all table schemas\n",
    "print(\"All Table Schemas\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table in TABLES:\n",
    "    if table_exists(table):\n",
    "        print(f\"\\n--- {table} ---\")\n",
    "        schema = get_schema(table)\n",
    "        for field in schema:\n",
    "            print(f\"  {field.name}: {field.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>__cdc_operation</th>\n",
       "      <th>__cdc_timestamp</th>\n",
       "      <th>__processed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Brown</td>\n",
       "      <td>charlie.brown@example.com</td>\n",
       "      <td>+1-555-0105</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-17 02:53:47+00:00</td>\n",
       "      <td>2025-12-17 03:32:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Williams</td>\n",
       "      <td>alice.williams@example.com</td>\n",
       "      <td>+1-555-0104</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-17 02:53:47+00:00</td>\n",
       "      <td>2025-12-17 03:32:38+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>bob.johnson@example.com</td>\n",
       "      <td>+1-555-0103</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-17 02:53:47+00:00</td>\n",
       "      <td>2025-12-17 03:32:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>jane.smith@example.com</td>\n",
       "      <td>+1-555-0102</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-17 02:53:47+00:00</td>\n",
       "      <td>2025-12-17 03:32:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>+1-555-0101</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>1970-01-21 10:32:19+00:00</td>\n",
       "      <td>r</td>\n",
       "      <td>2025-12-17 02:53:47+00:00</td>\n",
       "      <td>2025-12-17 03:32:27+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                       email        phone  \\\n",
       "0   5    Charlie     Brown   charlie.brown@example.com  +1-555-0105   \n",
       "1   4      Alice  Williams  alice.williams@example.com  +1-555-0104   \n",
       "2   3        Bob   Johnson     bob.johnson@example.com  +1-555-0103   \n",
       "3   2       Jane     Smith      jane.smith@example.com  +1-555-0102   \n",
       "4   1       John       Doe        john.doe@example.com  +1-555-0101   \n",
       "\n",
       "                 created_at                updated_at __cdc_operation  \\\n",
       "0 1970-01-21 10:32:19+00:00 1970-01-21 10:32:19+00:00               r   \n",
       "1 1970-01-21 10:32:19+00:00 1970-01-21 10:32:19+00:00               r   \n",
       "2 1970-01-21 10:32:19+00:00 1970-01-21 10:32:19+00:00               r   \n",
       "3 1970-01-21 10:32:19+00:00 1970-01-21 10:32:19+00:00               r   \n",
       "4 1970-01-21 10:32:19+00:00 1970-01-21 10:32:19+00:00               r   \n",
       "\n",
       "            __cdc_timestamp            __processed_at  \n",
       "0 2025-12-17 02:53:47+00:00 2025-12-17 03:32:39+00:00  \n",
       "1 2025-12-17 02:53:47+00:00 2025-12-17 03:32:38+00:00  \n",
       "2 2025-12-17 02:53:47+00:00 2025-12-17 03:32:36+00:00  \n",
       "3 2025-12-17 02:53:47+00:00 2025-12-17 03:32:32+00:00  \n",
       "4 2025-12-17 02:53:47+00:00 2025-12-17 03:32:27+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Custom query template\n",
    "# Modify the TABLE and add your own filters\n",
    "\n",
    "TABLE = \"customers\"  # Change table name\n",
    "\n",
    "if table_exists(TABLE):\n",
    "    df = load_table(TABLE)\n",
    "    \n",
    "    # Add your filters here using pandas\n",
    "    # Example: df_filtered = df[df['id'] > 5]\n",
    "    # Example: df_filtered = df[df['name'].str.contains('John')]\n",
    "    \n",
    "    display(df)\n",
    "else:\n",
    "    print(f\"Table '{TABLE}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export query results to CSV\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# TABLE = \"customers\"\n",
    "# OUTPUT_FILE = \"./query_results.csv\"\n",
    "\n",
    "# if table_exists(TABLE):\n",
    "#     df = load_table(TABLE)\n",
    "#     df.to_csv(OUTPUT_FILE, index=False)\n",
    "#     print(f\"Exported {len(df)} rows to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates two approaches to querying Delta Lake:\n",
    "\n",
    "### 1. **delta-rs (Python-native)**\n",
    "- **Sections:** 2-5\n",
    "- **Library:** `deltalake` Python package\n",
    "- **Pros:** \n",
    "  - Fast, lightweight (no JVM)\n",
    "  - Perfect for read-only operations\n",
    "  - Time travel support\n",
    "- **Cons:** \n",
    "  - Limited to reads, history, and metadata\n",
    "  - No SQL support\n",
    "  - No write operations\n",
    "- **Use when:** You need fast queries and don't need SQL or writes\n",
    "\n",
    "### 2. **PySpark SQL**\n",
    "- **Sections:** 6-6.1\n",
    "- **Library:** `pyspark` with Delta Lake\n",
    "- **Pros:** \n",
    "  - Full SQL support (SELECT, INSERT, UPDATE, DELETE, MERGE)\n",
    "  - Complex transformations and joins\n",
    "  - Schema evolution\n",
    "  - Write operations\n",
    "- **Cons:** \n",
    "  - Heavier (requires JVM)\n",
    "  - Python 3.14 requires SQL approach (not DataFrame API)\n",
    "- **Use when:** You need SQL queries, writes, or complex operations\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "**delta-rs:**\n",
    "```python\n",
    "# Load table (current version)\n",
    "df = load_table(\"customers\")\n",
    "\n",
    "# Load table (specific version)\n",
    "df = load_table(\"customers\", version=0)\n",
    "\n",
    "# Get table history\n",
    "history = get_history(\"customers\")\n",
    "\n",
    "# Check if table exists\n",
    "exists = table_exists(\"customers\")\n",
    "```\n",
    "\n",
    "**PySpark SQL:**\n",
    "```python\n",
    "# Query with SQL\n",
    "result = spark.sql(\"SELECT * FROM customers WHERE city = 'NYC'\")\n",
    "\n",
    "# Time travel\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path)\n",
    "\n",
    "# Write operations\n",
    "spark.sql(\"INSERT INTO customers VALUES (...)\")\n",
    "spark.sql(\"MERGE INTO customers ...\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltalake_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
