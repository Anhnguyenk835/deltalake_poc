services:
  # SOURCE DATABASE (PostgreSQL)
  postgres-source:
    image: postgres:15
    container_name: postgres-source
    hostname: postgres-source
    environment:
      POSTGRES_USER: source_user
      POSTGRES_PASSWORD: source_password
      POSTGRES_DB: source_db
    ports:
      - "5433:5432"
    volumes:
      - ./init-scripts/source-init.sql:/docker-entrypoint-initdb.d/init.sql
      - postgres_source_data:/var/lib/postgresql/data
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"                           #  Enable logical replication
      - "-c"
      - "max_wal_senders=4"                           #  Max concurrent replication connections          
      - "-c"
      - "max_replication_slots=4"                     #  Max replication slots
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U source_user -d source_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ZOOKEEPER (for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # KAFKA BROKER
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  # KAFKA CONNECT with DEBEZIUM
  debezium:
    image: debezium/connect:2.5
    container_name: debezium
    hostname: debezium
    depends_on:
      kafka:
        condition: service_healthy
      postgres-source:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 10s
      timeout: 10s
      retries: 10

  # TARGET DATABASE (PostgreSQL - Data Lake)
  postgres-datalake:
    image: postgres:15
    container_name: postgres-datalake
    hostname: postgres-datalake
    environment:
      POSTGRES_USER: datalake_user
      POSTGRES_PASSWORD: datalake_password
      POSTGRES_DB: datalake_db
    ports:
      - "5434:5432"
    volumes:
      - ./init-scripts/datalake-init.sql:/docker-entrypoint-initdb.d/init.sql
      - postgres_datalake_data:/var/lib/postgresql/data
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U datalake_user -d datalake_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  # TARGET DATABASE (PostgreSQL - Consumer/Target)
  postgres-target:
    image: postgres:15
    container_name: postgres-target
    hostname: postgres-target
    environment:
      POSTGRES_USER: target_user
      POSTGRES_PASSWORD: target_password
      POSTGRES_DB: target_db
    ports:
      - "5435:5432"
    volumes:
      - ./init-scripts/target-init.sql:/docker-entrypoint-initdb.d/init.sql
      - postgres_target_data:/var/lib/postgresql/data
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U target_user -d target_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  # KAFKA UI (Optional - for monitoring)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    hostname: kafka-ui
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: debezium
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://debezium:8083
    networks:
      - cdc-network

  # SPARK (Optional - for PySpark mode)
  # Using bde2020 spark images (free, actively maintained)
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    hostname: spark-master
    profiles: ["spark"]  # Only starts with: docker-compose --profile spark up
    environment:
      - INIT_DAEMON_STEP=setup_spark
    ports:
      - "7077:7077"
      - "8081:8080"   # Spark master UI
    volumes:
      - ./deltalake:/opt/spark-data/deltalake
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    hostname: spark-worker
    profiles: ["spark"]
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    ports:
      - "8082:8081"   # Spark worker UI
    volumes:
      - ./deltalake:/opt/spark-data/deltalake
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - cdc-network

  spark-streaming:
    build:
      context: ./consumer/spark-streaming
      dockerfile: Dockerfile.spark
    container_name: spark-streaming
    hostname: spark-streaming
    profiles: ["spark-streaming"]  # Start with: docker-compose --profile spark-streaming up
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - DELTA_LAKE_PATH=/opt/spark-data/deltalake
      - SPARK_CHECKPOINT_PATH=/opt/spark-data/checkpoints
      - SPARK_TRIGGER_INTERVAL=10 seconds
      - SPARK_MAX_OFFSETS_PER_TRIGGER=10000
      - SPARK_STARTING_OFFSETS=earliest
    ports:
      - "4040:4040"   # Spark UI for streaming job
    volumes:
      - ./deltalake:/opt/spark-data/deltalake
      - ./checkpoints:/opt/spark-data/checkpoints
    networks:
      - cdc-network
    restart: unless-stopped

  # ============================================
  # RISINGWAVE CONNECTOR (Streaming Database)
  # ============================================
  # Start with: docker-compose --profile risingwave up -d

  # MINIO (S3-compatible object storage for Delta Lake)
  minio:
    image: quay.io/minio/minio:latest
    container_name: minio
    hostname: minio
    profiles: ["risingwave"]
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    entrypoint: |
      /bin/sh -c '
      set -e
      mkdir -p /data/hummock001
      mkdir -p /data/deltalake
      /usr/bin/docker-entrypoint.sh "$$0" "$$@"
      '
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  # RISINGWAVE (Streaming Database)
  # Uses SQLite for metadata (simpler than etcd for POC)
  risingwave:
    image: risingwavelabs/risingwave:v2.5.1
    container_name: risingwave
    hostname: risingwave
    profiles: ["risingwave"]
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    command:
      - standalone
      - --meta-opts=--listen-addr 0.0.0.0:5690 --advertise-addr 0.0.0.0:5690 --dashboard-host 0.0.0.0:5691 --backend sql --sql-endpoint sqlite:///meta-data/metadata.db?mode=rwc --state-store hummock+minio://admin:password@minio:9000/hummock001 --data-directory hummock_001
      - --compute-opts=--listen-addr 0.0.0.0:5688 --advertise-addr 0.0.0.0:5688 --parallelism 4 --total-memory-bytes 2147483648 --role both --meta-address http://0.0.0.0:5690
      - --frontend-opts=--listen-addr 0.0.0.0:4566 --advertise-addr 0.0.0.0:4566 --meta-addr http://0.0.0.0:5690
      - --compactor-opts=--listen-addr 0.0.0.0:6660 --advertise-addr 0.0.0.0:6660 --meta-address http://0.0.0.0:5690
    ports:
      - "4566:4566"   # PostgreSQL-compatible SQL interface
      - "5691:5691"   # Dashboard UI
    volumes:
      - risingwave_meta:/meta-data
    environment:
      RUST_BACKTRACE: "1"
      ENABLE_TELEMETRY: "false"
    networks:
      - cdc-network
    healthcheck:
      test: ["CMD-SHELL", "bash -c '> /dev/tcp/127.0.0.1/4566; exit $$?;'"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 30s
    restart: unless-stopped

networks:
  cdc-network:
    driver: bridge

volumes:
  postgres_source_data:
  postgres_datalake_data:
  postgres_target_data:
  minio_data:
  risingwave_meta:
